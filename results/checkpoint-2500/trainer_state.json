{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0491803278688523,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004098360655737705,
      "grad_norm": 0.9527931213378906,
      "learning_rate": 0.00019978142076502732,
      "loss": 0.8217,
      "step": 5
    },
    {
      "epoch": 0.00819672131147541,
      "grad_norm": 0.5045624375343323,
      "learning_rate": 0.00019950819672131148,
      "loss": 0.6988,
      "step": 10
    },
    {
      "epoch": 0.012295081967213115,
      "grad_norm": 0.6213607788085938,
      "learning_rate": 0.00019923497267759562,
      "loss": 0.5741,
      "step": 15
    },
    {
      "epoch": 0.01639344262295082,
      "grad_norm": 1.107638955116272,
      "learning_rate": 0.0001989617486338798,
      "loss": 0.4477,
      "step": 20
    },
    {
      "epoch": 0.020491803278688523,
      "grad_norm": 0.4475697875022888,
      "learning_rate": 0.00019868852459016393,
      "loss": 0.2998,
      "step": 25
    },
    {
      "epoch": 0.02459016393442623,
      "grad_norm": 0.43611767888069153,
      "learning_rate": 0.0001984153005464481,
      "loss": 0.1331,
      "step": 30
    },
    {
      "epoch": 0.028688524590163935,
      "grad_norm": 0.9668943285942078,
      "learning_rate": 0.00019814207650273224,
      "loss": 0.1277,
      "step": 35
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 0.7083913683891296,
      "learning_rate": 0.0001978688524590164,
      "loss": 0.1006,
      "step": 40
    },
    {
      "epoch": 0.036885245901639344,
      "grad_norm": 0.8571495413780212,
      "learning_rate": 0.00019759562841530054,
      "loss": 0.0931,
      "step": 45
    },
    {
      "epoch": 0.040983606557377046,
      "grad_norm": 1.0128223896026611,
      "learning_rate": 0.0001973224043715847,
      "loss": 0.0423,
      "step": 50
    },
    {
      "epoch": 0.045081967213114756,
      "grad_norm": 0.185210719704628,
      "learning_rate": 0.00019704918032786885,
      "loss": 0.0145,
      "step": 55
    },
    {
      "epoch": 0.04918032786885246,
      "grad_norm": 0.2549950182437897,
      "learning_rate": 0.00019677595628415302,
      "loss": 0.0202,
      "step": 60
    },
    {
      "epoch": 0.05327868852459016,
      "grad_norm": 0.011635084636509418,
      "learning_rate": 0.00019650273224043716,
      "loss": 0.0239,
      "step": 65
    },
    {
      "epoch": 0.05737704918032787,
      "grad_norm": 0.07656217366456985,
      "learning_rate": 0.00019622950819672133,
      "loss": 0.0074,
      "step": 70
    },
    {
      "epoch": 0.06147540983606557,
      "grad_norm": 0.060497161000967026,
      "learning_rate": 0.00019595628415300547,
      "loss": 0.0276,
      "step": 75
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 0.07262999564409256,
      "learning_rate": 0.00019568306010928963,
      "loss": 0.0085,
      "step": 80
    },
    {
      "epoch": 0.06967213114754098,
      "grad_norm": 0.030240889638662338,
      "learning_rate": 0.00019540983606557377,
      "loss": 0.0083,
      "step": 85
    },
    {
      "epoch": 0.07377049180327869,
      "grad_norm": 0.04026243835687637,
      "learning_rate": 0.00019513661202185794,
      "loss": 0.0052,
      "step": 90
    },
    {
      "epoch": 0.0778688524590164,
      "grad_norm": 0.19012518227100372,
      "learning_rate": 0.00019486338797814208,
      "loss": 0.0061,
      "step": 95
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 0.16719123721122742,
      "learning_rate": 0.00019459016393442625,
      "loss": 0.0101,
      "step": 100
    },
    {
      "epoch": 0.0860655737704918,
      "grad_norm": 0.017311835661530495,
      "learning_rate": 0.0001943169398907104,
      "loss": 0.0034,
      "step": 105
    },
    {
      "epoch": 0.09016393442622951,
      "grad_norm": 0.021288778632879257,
      "learning_rate": 0.00019404371584699455,
      "loss": 0.0078,
      "step": 110
    },
    {
      "epoch": 0.0942622950819672,
      "grad_norm": 0.0617251843214035,
      "learning_rate": 0.00019377049180327872,
      "loss": 0.0026,
      "step": 115
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 0.8364461660385132,
      "learning_rate": 0.00019349726775956286,
      "loss": 0.0068,
      "step": 120
    },
    {
      "epoch": 0.10245901639344263,
      "grad_norm": 0.0058639030903577805,
      "learning_rate": 0.000193224043715847,
      "loss": 0.0041,
      "step": 125
    },
    {
      "epoch": 0.10655737704918032,
      "grad_norm": 0.04045857861638069,
      "learning_rate": 0.00019295081967213117,
      "loss": 0.0072,
      "step": 130
    },
    {
      "epoch": 0.11065573770491803,
      "grad_norm": 1.3383764028549194,
      "learning_rate": 0.0001926775956284153,
      "loss": 0.0182,
      "step": 135
    },
    {
      "epoch": 0.11475409836065574,
      "grad_norm": 0.1323436200618744,
      "learning_rate": 0.00019240437158469945,
      "loss": 0.0021,
      "step": 140
    },
    {
      "epoch": 0.11885245901639344,
      "grad_norm": 0.025622591376304626,
      "learning_rate": 0.00019213114754098362,
      "loss": 0.0046,
      "step": 145
    },
    {
      "epoch": 0.12295081967213115,
      "grad_norm": 0.010446264408528805,
      "learning_rate": 0.00019185792349726776,
      "loss": 0.0027,
      "step": 150
    },
    {
      "epoch": 0.12704918032786885,
      "grad_norm": 0.03192673996090889,
      "learning_rate": 0.00019158469945355192,
      "loss": 0.0023,
      "step": 155
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 0.009083353914320469,
      "learning_rate": 0.00019131147540983606,
      "loss": 0.0012,
      "step": 160
    },
    {
      "epoch": 0.13524590163934427,
      "grad_norm": 0.01632789522409439,
      "learning_rate": 0.00019103825136612023,
      "loss": 0.002,
      "step": 165
    },
    {
      "epoch": 0.13934426229508196,
      "grad_norm": 0.03792854771018028,
      "learning_rate": 0.00019076502732240437,
      "loss": 0.0018,
      "step": 170
    },
    {
      "epoch": 0.14344262295081966,
      "grad_norm": 0.049662161618471146,
      "learning_rate": 0.00019049180327868854,
      "loss": 0.0056,
      "step": 175
    },
    {
      "epoch": 0.14754098360655737,
      "grad_norm": 0.004366317763924599,
      "learning_rate": 0.00019021857923497268,
      "loss": 0.0045,
      "step": 180
    },
    {
      "epoch": 0.15163934426229508,
      "grad_norm": 0.0021171499975025654,
      "learning_rate": 0.00018994535519125684,
      "loss": 0.0047,
      "step": 185
    },
    {
      "epoch": 0.1557377049180328,
      "grad_norm": 0.04822837933897972,
      "learning_rate": 0.00018967213114754098,
      "loss": 0.0016,
      "step": 190
    },
    {
      "epoch": 0.1598360655737705,
      "grad_norm": 0.01301040593534708,
      "learning_rate": 0.00018939890710382515,
      "loss": 0.001,
      "step": 195
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 0.09617028385400772,
      "learning_rate": 0.0001891256830601093,
      "loss": 0.0022,
      "step": 200
    },
    {
      "epoch": 0.1680327868852459,
      "grad_norm": 0.0021836638916283846,
      "learning_rate": 0.00018885245901639346,
      "loss": 0.0015,
      "step": 205
    },
    {
      "epoch": 0.1721311475409836,
      "grad_norm": 0.017412008717656136,
      "learning_rate": 0.0001885792349726776,
      "loss": 0.0021,
      "step": 210
    },
    {
      "epoch": 0.1762295081967213,
      "grad_norm": 0.008060822263360023,
      "learning_rate": 0.00018830601092896177,
      "loss": 0.0007,
      "step": 215
    },
    {
      "epoch": 0.18032786885245902,
      "grad_norm": 0.02488826774060726,
      "learning_rate": 0.0001880327868852459,
      "loss": 0.0024,
      "step": 220
    },
    {
      "epoch": 0.18442622950819673,
      "grad_norm": 0.005369509104639292,
      "learning_rate": 0.00018775956284153007,
      "loss": 0.0017,
      "step": 225
    },
    {
      "epoch": 0.1885245901639344,
      "grad_norm": 0.02311077155172825,
      "learning_rate": 0.0001874863387978142,
      "loss": 0.001,
      "step": 230
    },
    {
      "epoch": 0.19262295081967212,
      "grad_norm": 0.018028857186436653,
      "learning_rate": 0.00018721311475409838,
      "loss": 0.0024,
      "step": 235
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 0.004582926630973816,
      "learning_rate": 0.00018693989071038252,
      "loss": 0.0015,
      "step": 240
    },
    {
      "epoch": 0.20081967213114754,
      "grad_norm": 0.01514498796314001,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.0022,
      "step": 245
    },
    {
      "epoch": 0.20491803278688525,
      "grad_norm": 0.009839089587330818,
      "learning_rate": 0.00018639344262295083,
      "loss": 0.0053,
      "step": 250
    },
    {
      "epoch": 0.20901639344262296,
      "grad_norm": 1.1162108182907104,
      "learning_rate": 0.000186120218579235,
      "loss": 0.0089,
      "step": 255
    },
    {
      "epoch": 0.21311475409836064,
      "grad_norm": 0.009776981547474861,
      "learning_rate": 0.00018584699453551913,
      "loss": 0.001,
      "step": 260
    },
    {
      "epoch": 0.21721311475409835,
      "grad_norm": 0.0063355970196425915,
      "learning_rate": 0.0001855737704918033,
      "loss": 0.0017,
      "step": 265
    },
    {
      "epoch": 0.22131147540983606,
      "grad_norm": 0.005864137317985296,
      "learning_rate": 0.00018530054644808744,
      "loss": 0.0018,
      "step": 270
    },
    {
      "epoch": 0.22540983606557377,
      "grad_norm": 0.07048726826906204,
      "learning_rate": 0.0001850273224043716,
      "loss": 0.0019,
      "step": 275
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 0.02692907676100731,
      "learning_rate": 0.00018475409836065575,
      "loss": 0.0012,
      "step": 280
    },
    {
      "epoch": 0.2336065573770492,
      "grad_norm": 0.01985354721546173,
      "learning_rate": 0.0001844808743169399,
      "loss": 0.0015,
      "step": 285
    },
    {
      "epoch": 0.23770491803278687,
      "grad_norm": 0.002445412566885352,
      "learning_rate": 0.00018420765027322405,
      "loss": 0.0012,
      "step": 290
    },
    {
      "epoch": 0.24180327868852458,
      "grad_norm": 0.0151666896417737,
      "learning_rate": 0.0001839344262295082,
      "loss": 0.0008,
      "step": 295
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 0.008400567807257175,
      "learning_rate": 0.00018366120218579236,
      "loss": 0.0022,
      "step": 300
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01348680816590786,
      "learning_rate": 0.0001833879781420765,
      "loss": 0.0038,
      "step": 305
    },
    {
      "epoch": 0.2540983606557377,
      "grad_norm": 0.008146454580128193,
      "learning_rate": 0.00018311475409836067,
      "loss": 0.0006,
      "step": 310
    },
    {
      "epoch": 0.2581967213114754,
      "grad_norm": 0.07796267420053482,
      "learning_rate": 0.0001828415300546448,
      "loss": 0.0021,
      "step": 315
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 0.1860826313495636,
      "learning_rate": 0.00018256830601092898,
      "loss": 0.0072,
      "step": 320
    },
    {
      "epoch": 0.26639344262295084,
      "grad_norm": 0.0037070682737976313,
      "learning_rate": 0.00018229508196721312,
      "loss": 0.0012,
      "step": 325
    },
    {
      "epoch": 0.27049180327868855,
      "grad_norm": 0.001117024919949472,
      "learning_rate": 0.00018202185792349728,
      "loss": 0.0004,
      "step": 330
    },
    {
      "epoch": 0.27459016393442626,
      "grad_norm": 0.1939241886138916,
      "learning_rate": 0.00018174863387978142,
      "loss": 0.0063,
      "step": 335
    },
    {
      "epoch": 0.2786885245901639,
      "grad_norm": 0.020207803696393967,
      "learning_rate": 0.0001814754098360656,
      "loss": 0.001,
      "step": 340
    },
    {
      "epoch": 0.2827868852459016,
      "grad_norm": 0.010100802406668663,
      "learning_rate": 0.00018120218579234973,
      "loss": 0.0004,
      "step": 345
    },
    {
      "epoch": 0.28688524590163933,
      "grad_norm": 0.012284435331821442,
      "learning_rate": 0.0001809289617486339,
      "loss": 0.0047,
      "step": 350
    },
    {
      "epoch": 0.29098360655737704,
      "grad_norm": 0.026365771889686584,
      "learning_rate": 0.00018065573770491804,
      "loss": 0.0011,
      "step": 355
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 0.0016114693135023117,
      "learning_rate": 0.0001803825136612022,
      "loss": 0.001,
      "step": 360
    },
    {
      "epoch": 0.29918032786885246,
      "grad_norm": 0.0032059005461633205,
      "learning_rate": 0.00018010928961748634,
      "loss": 0.0023,
      "step": 365
    },
    {
      "epoch": 0.30327868852459017,
      "grad_norm": 0.055000223219394684,
      "learning_rate": 0.0001798360655737705,
      "loss": 0.0019,
      "step": 370
    },
    {
      "epoch": 0.3073770491803279,
      "grad_norm": 0.0024290892761200666,
      "learning_rate": 0.00017956284153005465,
      "loss": 0.0009,
      "step": 375
    },
    {
      "epoch": 0.3114754098360656,
      "grad_norm": 0.016742771491408348,
      "learning_rate": 0.00017928961748633882,
      "loss": 0.0008,
      "step": 380
    },
    {
      "epoch": 0.3155737704918033,
      "grad_norm": 0.002140327822417021,
      "learning_rate": 0.00017901639344262296,
      "loss": 0.0007,
      "step": 385
    },
    {
      "epoch": 0.319672131147541,
      "grad_norm": 0.020881978794932365,
      "learning_rate": 0.00017874316939890713,
      "loss": 0.0037,
      "step": 390
    },
    {
      "epoch": 0.3237704918032787,
      "grad_norm": 0.007238979451358318,
      "learning_rate": 0.00017846994535519127,
      "loss": 0.0009,
      "step": 395
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 0.10407977551221848,
      "learning_rate": 0.00017819672131147543,
      "loss": 0.0014,
      "step": 400
    },
    {
      "epoch": 0.3319672131147541,
      "grad_norm": 0.003155569778755307,
      "learning_rate": 0.00017792349726775957,
      "loss": 0.0005,
      "step": 405
    },
    {
      "epoch": 0.3360655737704918,
      "grad_norm": 0.006420270539820194,
      "learning_rate": 0.00017765027322404374,
      "loss": 0.0005,
      "step": 410
    },
    {
      "epoch": 0.3401639344262295,
      "grad_norm": 0.0073453523218631744,
      "learning_rate": 0.00017737704918032788,
      "loss": 0.0005,
      "step": 415
    },
    {
      "epoch": 0.3442622950819672,
      "grad_norm": 0.015700800344347954,
      "learning_rate": 0.00017710382513661205,
      "loss": 0.0011,
      "step": 420
    },
    {
      "epoch": 0.3483606557377049,
      "grad_norm": 0.04038052260875702,
      "learning_rate": 0.0001768306010928962,
      "loss": 0.0012,
      "step": 425
    },
    {
      "epoch": 0.3524590163934426,
      "grad_norm": 0.0008535035303793848,
      "learning_rate": 0.00017655737704918033,
      "loss": 0.0008,
      "step": 430
    },
    {
      "epoch": 0.35655737704918034,
      "grad_norm": 0.00859803706407547,
      "learning_rate": 0.0001762841530054645,
      "loss": 0.0008,
      "step": 435
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 0.44438108801841736,
      "learning_rate": 0.00017601092896174863,
      "loss": 0.0068,
      "step": 440
    },
    {
      "epoch": 0.36475409836065575,
      "grad_norm": 0.014533805660903454,
      "learning_rate": 0.00017573770491803277,
      "loss": 0.0007,
      "step": 445
    },
    {
      "epoch": 0.36885245901639346,
      "grad_norm": 0.005178009159862995,
      "learning_rate": 0.00017546448087431694,
      "loss": 0.0006,
      "step": 450
    },
    {
      "epoch": 0.3729508196721312,
      "grad_norm": 0.04526493698358536,
      "learning_rate": 0.00017519125683060108,
      "loss": 0.0006,
      "step": 455
    },
    {
      "epoch": 0.3770491803278688,
      "grad_norm": 0.011716095730662346,
      "learning_rate": 0.00017491803278688525,
      "loss": 0.002,
      "step": 460
    },
    {
      "epoch": 0.38114754098360654,
      "grad_norm": 0.0009423488518223166,
      "learning_rate": 0.0001746448087431694,
      "loss": 0.0032,
      "step": 465
    },
    {
      "epoch": 0.38524590163934425,
      "grad_norm": 0.0077596502378582954,
      "learning_rate": 0.00017437158469945356,
      "loss": 0.0011,
      "step": 470
    },
    {
      "epoch": 0.38934426229508196,
      "grad_norm": 0.0035024548415094614,
      "learning_rate": 0.0001740983606557377,
      "loss": 0.0004,
      "step": 475
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 0.18749438226222992,
      "learning_rate": 0.00017382513661202186,
      "loss": 0.0021,
      "step": 480
    },
    {
      "epoch": 0.3975409836065574,
      "grad_norm": 0.0100320503115654,
      "learning_rate": 0.00017355191256830603,
      "loss": 0.0008,
      "step": 485
    },
    {
      "epoch": 0.4016393442622951,
      "grad_norm": 0.0029418293852359056,
      "learning_rate": 0.00017327868852459017,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 0.4057377049180328,
      "grad_norm": 0.0038733775727450848,
      "learning_rate": 0.00017300546448087434,
      "loss": 0.0011,
      "step": 495
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 0.014742834493517876,
      "learning_rate": 0.00017273224043715848,
      "loss": 0.0006,
      "step": 500
    },
    {
      "epoch": 0.4139344262295082,
      "grad_norm": 0.001188283902592957,
      "learning_rate": 0.00017245901639344264,
      "loss": 0.0004,
      "step": 505
    },
    {
      "epoch": 0.4180327868852459,
      "grad_norm": 0.007202642969787121,
      "learning_rate": 0.00017218579234972678,
      "loss": 0.0009,
      "step": 510
    },
    {
      "epoch": 0.42213114754098363,
      "grad_norm": 0.0009368335595354438,
      "learning_rate": 0.00017191256830601095,
      "loss": 0.0006,
      "step": 515
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 0.04385055974125862,
      "learning_rate": 0.0001716393442622951,
      "loss": 0.001,
      "step": 520
    },
    {
      "epoch": 0.430327868852459,
      "grad_norm": 0.006209665909409523,
      "learning_rate": 0.00017136612021857926,
      "loss": 0.0012,
      "step": 525
    },
    {
      "epoch": 0.4344262295081967,
      "grad_norm": 0.0004584489797707647,
      "learning_rate": 0.0001710928961748634,
      "loss": 0.0011,
      "step": 530
    },
    {
      "epoch": 0.4385245901639344,
      "grad_norm": 0.15696106851100922,
      "learning_rate": 0.00017081967213114757,
      "loss": 0.0015,
      "step": 535
    },
    {
      "epoch": 0.4426229508196721,
      "grad_norm": 0.001240383367985487,
      "learning_rate": 0.0001705464480874317,
      "loss": 0.0005,
      "step": 540
    },
    {
      "epoch": 0.44672131147540983,
      "grad_norm": 0.025476330891251564,
      "learning_rate": 0.00017027322404371587,
      "loss": 0.0007,
      "step": 545
    },
    {
      "epoch": 0.45081967213114754,
      "grad_norm": 0.002015160396695137,
      "learning_rate": 0.00017,
      "loss": 0.0006,
      "step": 550
    },
    {
      "epoch": 0.45491803278688525,
      "grad_norm": 0.015847815200686455,
      "learning_rate": 0.00016972677595628418,
      "loss": 0.0005,
      "step": 555
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 0.004574120976030827,
      "learning_rate": 0.00016945355191256832,
      "loss": 0.001,
      "step": 560
    },
    {
      "epoch": 0.46311475409836067,
      "grad_norm": 0.0032532610930502415,
      "learning_rate": 0.00016918032786885249,
      "loss": 0.0004,
      "step": 565
    },
    {
      "epoch": 0.4672131147540984,
      "grad_norm": 0.006321570836007595,
      "learning_rate": 0.00016890710382513663,
      "loss": 0.0007,
      "step": 570
    },
    {
      "epoch": 0.4713114754098361,
      "grad_norm": 0.010698975063860416,
      "learning_rate": 0.00016863387978142077,
      "loss": 0.0008,
      "step": 575
    },
    {
      "epoch": 0.47540983606557374,
      "grad_norm": 0.005457952152937651,
      "learning_rate": 0.0001683606557377049,
      "loss": 0.0005,
      "step": 580
    },
    {
      "epoch": 0.47950819672131145,
      "grad_norm": 0.0009932118700817227,
      "learning_rate": 0.00016808743169398907,
      "loss": 0.0014,
      "step": 585
    },
    {
      "epoch": 0.48360655737704916,
      "grad_norm": 0.0011652065441012383,
      "learning_rate": 0.0001678142076502732,
      "loss": 0.0094,
      "step": 590
    },
    {
      "epoch": 0.48770491803278687,
      "grad_norm": 0.001046116347424686,
      "learning_rate": 0.00016754098360655738,
      "loss": 0.0004,
      "step": 595
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 0.0011388229904696345,
      "learning_rate": 0.00016726775956284152,
      "loss": 0.0004,
      "step": 600
    },
    {
      "epoch": 0.4959016393442623,
      "grad_norm": 0.009055097587406635,
      "learning_rate": 0.0001669945355191257,
      "loss": 0.0007,
      "step": 605
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.006677847355604172,
      "learning_rate": 0.00016672131147540983,
      "loss": 0.0005,
      "step": 610
    },
    {
      "epoch": 0.5040983606557377,
      "grad_norm": 0.0036536981351673603,
      "learning_rate": 0.000166448087431694,
      "loss": 0.0011,
      "step": 615
    },
    {
      "epoch": 0.5081967213114754,
      "grad_norm": 0.01242875587195158,
      "learning_rate": 0.00016617486338797813,
      "loss": 0.0005,
      "step": 620
    },
    {
      "epoch": 0.5122950819672131,
      "grad_norm": 0.002459513023495674,
      "learning_rate": 0.0001659016393442623,
      "loss": 0.0004,
      "step": 625
    },
    {
      "epoch": 0.5163934426229508,
      "grad_norm": 0.012614401988685131,
      "learning_rate": 0.00016562841530054644,
      "loss": 0.0007,
      "step": 630
    },
    {
      "epoch": 0.5204918032786885,
      "grad_norm": 0.017447732388973236,
      "learning_rate": 0.0001653551912568306,
      "loss": 0.0007,
      "step": 635
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 0.08051905035972595,
      "learning_rate": 0.00016508196721311475,
      "loss": 0.0025,
      "step": 640
    },
    {
      "epoch": 0.5286885245901639,
      "grad_norm": 0.03263995796442032,
      "learning_rate": 0.00016480874316939892,
      "loss": 0.0007,
      "step": 645
    },
    {
      "epoch": 0.5327868852459017,
      "grad_norm": 0.0012347821611911058,
      "learning_rate": 0.00016453551912568306,
      "loss": 0.0016,
      "step": 650
    },
    {
      "epoch": 0.5368852459016393,
      "grad_norm": 0.0004190754843875766,
      "learning_rate": 0.00016426229508196722,
      "loss": 0.0009,
      "step": 655
    },
    {
      "epoch": 0.5409836065573771,
      "grad_norm": 0.0014187332708388567,
      "learning_rate": 0.0001639890710382514,
      "loss": 0.0027,
      "step": 660
    },
    {
      "epoch": 0.5450819672131147,
      "grad_norm": 0.0004880629130639136,
      "learning_rate": 0.00016371584699453553,
      "loss": 0.0005,
      "step": 665
    },
    {
      "epoch": 0.5491803278688525,
      "grad_norm": 0.003649368416517973,
      "learning_rate": 0.0001634426229508197,
      "loss": 0.0003,
      "step": 670
    },
    {
      "epoch": 0.5532786885245902,
      "grad_norm": 0.0032825381495058537,
      "learning_rate": 0.00016316939890710384,
      "loss": 0.0004,
      "step": 675
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 0.0959416851401329,
      "learning_rate": 0.000162896174863388,
      "loss": 0.0035,
      "step": 680
    },
    {
      "epoch": 0.5614754098360656,
      "grad_norm": 0.007344976533204317,
      "learning_rate": 0.00016262295081967214,
      "loss": 0.0003,
      "step": 685
    },
    {
      "epoch": 0.5655737704918032,
      "grad_norm": 0.007972766645252705,
      "learning_rate": 0.0001623497267759563,
      "loss": 0.0022,
      "step": 690
    },
    {
      "epoch": 0.569672131147541,
      "grad_norm": 0.01102761272341013,
      "learning_rate": 0.00016207650273224045,
      "loss": 0.0004,
      "step": 695
    },
    {
      "epoch": 0.5737704918032787,
      "grad_norm": 0.010044329799711704,
      "learning_rate": 0.00016180327868852462,
      "loss": 0.0007,
      "step": 700
    },
    {
      "epoch": 0.5778688524590164,
      "grad_norm": 0.001592421205714345,
      "learning_rate": 0.00016153005464480876,
      "loss": 0.0004,
      "step": 705
    },
    {
      "epoch": 0.5819672131147541,
      "grad_norm": 0.00233217841014266,
      "learning_rate": 0.0001612568306010929,
      "loss": 0.0006,
      "step": 710
    },
    {
      "epoch": 0.5860655737704918,
      "grad_norm": 0.0016034804284572601,
      "learning_rate": 0.00016098360655737707,
      "loss": 0.0003,
      "step": 715
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 0.0024701934307813644,
      "learning_rate": 0.0001607103825136612,
      "loss": 0.0004,
      "step": 720
    },
    {
      "epoch": 0.5942622950819673,
      "grad_norm": 0.0340757817029953,
      "learning_rate": 0.00016043715846994535,
      "loss": 0.0007,
      "step": 725
    },
    {
      "epoch": 0.5983606557377049,
      "grad_norm": 0.00766187347471714,
      "learning_rate": 0.0001601639344262295,
      "loss": 0.0013,
      "step": 730
    },
    {
      "epoch": 0.6024590163934426,
      "grad_norm": 0.0009879113640636206,
      "learning_rate": 0.00015989071038251365,
      "loss": 0.0025,
      "step": 735
    },
    {
      "epoch": 0.6065573770491803,
      "grad_norm": 0.016811899840831757,
      "learning_rate": 0.00015961748633879782,
      "loss": 0.0004,
      "step": 740
    },
    {
      "epoch": 0.610655737704918,
      "grad_norm": 0.0063027977012097836,
      "learning_rate": 0.00015934426229508196,
      "loss": 0.0004,
      "step": 745
    },
    {
      "epoch": 0.6147540983606558,
      "grad_norm": 0.0037851082161068916,
      "learning_rate": 0.00015907103825136613,
      "loss": 0.0008,
      "step": 750
    },
    {
      "epoch": 0.6188524590163934,
      "grad_norm": 0.05159326642751694,
      "learning_rate": 0.00015879781420765027,
      "loss": 0.0005,
      "step": 755
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 0.01401988323777914,
      "learning_rate": 0.00015852459016393443,
      "loss": 0.0004,
      "step": 760
    },
    {
      "epoch": 0.6270491803278688,
      "grad_norm": 0.00021337141515687108,
      "learning_rate": 0.00015825136612021857,
      "loss": 0.0002,
      "step": 765
    },
    {
      "epoch": 0.6311475409836066,
      "grad_norm": 0.00018688617274165154,
      "learning_rate": 0.00015797814207650274,
      "loss": 0.0005,
      "step": 770
    },
    {
      "epoch": 0.6352459016393442,
      "grad_norm": 0.0009013944072648883,
      "learning_rate": 0.00015770491803278688,
      "loss": 0.0002,
      "step": 775
    },
    {
      "epoch": 0.639344262295082,
      "grad_norm": 0.09106282889842987,
      "learning_rate": 0.00015743169398907105,
      "loss": 0.0009,
      "step": 780
    },
    {
      "epoch": 0.6434426229508197,
      "grad_norm": 0.0009247095440514386,
      "learning_rate": 0.0001571584699453552,
      "loss": 0.001,
      "step": 785
    },
    {
      "epoch": 0.6475409836065574,
      "grad_norm": 0.03326667100191116,
      "learning_rate": 0.00015688524590163936,
      "loss": 0.0005,
      "step": 790
    },
    {
      "epoch": 0.6516393442622951,
      "grad_norm": 0.00021822088456247002,
      "learning_rate": 0.0001566120218579235,
      "loss": 0.0003,
      "step": 795
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 0.014621775597333908,
      "learning_rate": 0.00015633879781420766,
      "loss": 0.002,
      "step": 800
    },
    {
      "epoch": 0.6598360655737705,
      "grad_norm": 0.009806141257286072,
      "learning_rate": 0.0001560655737704918,
      "loss": 0.0003,
      "step": 805
    },
    {
      "epoch": 0.6639344262295082,
      "grad_norm": 0.0009548774687573314,
      "learning_rate": 0.00015579234972677597,
      "loss": 0.0007,
      "step": 810
    },
    {
      "epoch": 0.6680327868852459,
      "grad_norm": 0.001779852551408112,
      "learning_rate": 0.0001555191256830601,
      "loss": 0.0004,
      "step": 815
    },
    {
      "epoch": 0.6721311475409836,
      "grad_norm": 0.004381593316793442,
      "learning_rate": 0.00015524590163934428,
      "loss": 0.0009,
      "step": 820
    },
    {
      "epoch": 0.6762295081967213,
      "grad_norm": 0.004107135348021984,
      "learning_rate": 0.00015497267759562842,
      "loss": 0.0006,
      "step": 825
    },
    {
      "epoch": 0.680327868852459,
      "grad_norm": 0.001672861399129033,
      "learning_rate": 0.00015469945355191258,
      "loss": 0.0009,
      "step": 830
    },
    {
      "epoch": 0.6844262295081968,
      "grad_norm": 0.009800495579838753,
      "learning_rate": 0.00015442622950819672,
      "loss": 0.0007,
      "step": 835
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 0.0028825171757489443,
      "learning_rate": 0.0001541530054644809,
      "loss": 0.0002,
      "step": 840
    },
    {
      "epoch": 0.6926229508196722,
      "grad_norm": 0.002611017320305109,
      "learning_rate": 0.00015387978142076506,
      "loss": 0.0003,
      "step": 845
    },
    {
      "epoch": 0.6967213114754098,
      "grad_norm": 0.011444499716162682,
      "learning_rate": 0.0001536065573770492,
      "loss": 0.0014,
      "step": 850
    },
    {
      "epoch": 0.7008196721311475,
      "grad_norm": 0.002013763878494501,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.0004,
      "step": 855
    },
    {
      "epoch": 0.7049180327868853,
      "grad_norm": 0.01398982759565115,
      "learning_rate": 0.0001530601092896175,
      "loss": 0.0005,
      "step": 860
    },
    {
      "epoch": 0.7090163934426229,
      "grad_norm": 0.0059020547196269035,
      "learning_rate": 0.00015278688524590165,
      "loss": 0.0005,
      "step": 865
    },
    {
      "epoch": 0.7131147540983607,
      "grad_norm": 0.0,
      "learning_rate": 0.00015251366120218579,
      "loss": 0.0004,
      "step": 870
    },
    {
      "epoch": 0.7172131147540983,
      "grad_norm": 0.00707361102104187,
      "learning_rate": 0.00015224043715846995,
      "loss": 0.0003,
      "step": 875
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 0.006944413762539625,
      "learning_rate": 0.0001519672131147541,
      "loss": 0.0008,
      "step": 880
    },
    {
      "epoch": 0.7254098360655737,
      "grad_norm": 0.012478955090045929,
      "learning_rate": 0.00015169398907103826,
      "loss": 0.0003,
      "step": 885
    },
    {
      "epoch": 0.7295081967213115,
      "grad_norm": 0.00017574291268829256,
      "learning_rate": 0.0001514207650273224,
      "loss": 0.0006,
      "step": 890
    },
    {
      "epoch": 0.7336065573770492,
      "grad_norm": 0.002650270704180002,
      "learning_rate": 0.00015114754098360657,
      "loss": 0.0002,
      "step": 895
    },
    {
      "epoch": 0.7377049180327869,
      "grad_norm": 0.0011695773573592305,
      "learning_rate": 0.0001508743169398907,
      "loss": 0.0004,
      "step": 900
    },
    {
      "epoch": 0.7418032786885246,
      "grad_norm": 0.0007104747928678989,
      "learning_rate": 0.00015060109289617487,
      "loss": 0.0003,
      "step": 905
    },
    {
      "epoch": 0.7459016393442623,
      "grad_norm": 0.013166557997465134,
      "learning_rate": 0.000150327868852459,
      "loss": 0.0005,
      "step": 910
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.002409046981483698,
      "learning_rate": 0.00015005464480874318,
      "loss": 0.0002,
      "step": 915
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 0.0009846242610365152,
      "learning_rate": 0.00014978142076502732,
      "loss": 0.0016,
      "step": 920
    },
    {
      "epoch": 0.7581967213114754,
      "grad_norm": 0.006381461396813393,
      "learning_rate": 0.0001495081967213115,
      "loss": 0.0003,
      "step": 925
    },
    {
      "epoch": 0.7622950819672131,
      "grad_norm": 0.014935264363884926,
      "learning_rate": 0.00014923497267759563,
      "loss": 0.0005,
      "step": 930
    },
    {
      "epoch": 0.7663934426229508,
      "grad_norm": 2.8892562113469467e-05,
      "learning_rate": 0.0001489617486338798,
      "loss": 0.0002,
      "step": 935
    },
    {
      "epoch": 0.7704918032786885,
      "grad_norm": 5.2531999244820327e-05,
      "learning_rate": 0.00014868852459016393,
      "loss": 0.0002,
      "step": 940
    },
    {
      "epoch": 0.7745901639344263,
      "grad_norm": 0.0003357222012709826,
      "learning_rate": 0.0001484153005464481,
      "loss": 0.0006,
      "step": 945
    },
    {
      "epoch": 0.7786885245901639,
      "grad_norm": 0.0025565708056092262,
      "learning_rate": 0.00014814207650273224,
      "loss": 0.0003,
      "step": 950
    },
    {
      "epoch": 0.7827868852459017,
      "grad_norm": 6.038168794475496e-05,
      "learning_rate": 0.0001478688524590164,
      "loss": 0.0001,
      "step": 955
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 0.0017571707721799612,
      "learning_rate": 0.00014759562841530055,
      "loss": 0.0003,
      "step": 960
    },
    {
      "epoch": 0.7909836065573771,
      "grad_norm": 0.013048162683844566,
      "learning_rate": 0.00014732240437158472,
      "loss": 0.0039,
      "step": 965
    },
    {
      "epoch": 0.7950819672131147,
      "grad_norm": 6.025774382578675e-06,
      "learning_rate": 0.00014704918032786886,
      "loss": 0.0003,
      "step": 970
    },
    {
      "epoch": 0.7991803278688525,
      "grad_norm": 0.0026503284461796284,
      "learning_rate": 0.00014677595628415302,
      "loss": 0.0003,
      "step": 975
    },
    {
      "epoch": 0.8032786885245902,
      "grad_norm": 0.0007452239515259862,
      "learning_rate": 0.00014650273224043716,
      "loss": 0.0003,
      "step": 980
    },
    {
      "epoch": 0.8073770491803278,
      "grad_norm": 0.00042176988790743053,
      "learning_rate": 0.00014622950819672133,
      "loss": 0.0003,
      "step": 985
    },
    {
      "epoch": 0.8114754098360656,
      "grad_norm": 0.006051498930901289,
      "learning_rate": 0.00014595628415300547,
      "loss": 0.0004,
      "step": 990
    },
    {
      "epoch": 0.8155737704918032,
      "grad_norm": 0.001158890314400196,
      "learning_rate": 0.00014568306010928964,
      "loss": 0.0003,
      "step": 995
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 0.009514193050563335,
      "learning_rate": 0.00014540983606557378,
      "loss": 0.0004,
      "step": 1000
    },
    {
      "epoch": 0.8237704918032787,
      "grad_norm": 0.006030993536114693,
      "learning_rate": 0.00014513661202185794,
      "loss": 0.0003,
      "step": 1005
    },
    {
      "epoch": 0.8278688524590164,
      "grad_norm": 0.0015964112244546413,
      "learning_rate": 0.00014486338797814208,
      "loss": 0.0003,
      "step": 1010
    },
    {
      "epoch": 0.8319672131147541,
      "grad_norm": 0.00158079678658396,
      "learning_rate": 0.00014459016393442622,
      "loss": 0.0003,
      "step": 1015
    },
    {
      "epoch": 0.8360655737704918,
      "grad_norm": 0.0019842267502099276,
      "learning_rate": 0.0001443169398907104,
      "loss": 0.0004,
      "step": 1020
    },
    {
      "epoch": 0.8401639344262295,
      "grad_norm": 0.00946321152150631,
      "learning_rate": 0.00014404371584699453,
      "loss": 0.0003,
      "step": 1025
    },
    {
      "epoch": 0.8442622950819673,
      "grad_norm": 0.0010287526529282331,
      "learning_rate": 0.0001437704918032787,
      "loss": 0.0005,
      "step": 1030
    },
    {
      "epoch": 0.8483606557377049,
      "grad_norm": 0.00543291587382555,
      "learning_rate": 0.00014349726775956284,
      "loss": 0.0002,
      "step": 1035
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 0.00014987921167630702,
      "learning_rate": 0.000143224043715847,
      "loss": 0.0006,
      "step": 1040
    },
    {
      "epoch": 0.8565573770491803,
      "grad_norm": 0.001962999813258648,
      "learning_rate": 0.00014295081967213115,
      "loss": 0.0003,
      "step": 1045
    },
    {
      "epoch": 0.860655737704918,
      "grad_norm": 0.008594807237386703,
      "learning_rate": 0.0001426775956284153,
      "loss": 0.0009,
      "step": 1050
    },
    {
      "epoch": 0.8647540983606558,
      "grad_norm": 0.0005910912877880037,
      "learning_rate": 0.00014240437158469945,
      "loss": 0.0003,
      "step": 1055
    },
    {
      "epoch": 0.8688524590163934,
      "grad_norm": 0.006622785702347755,
      "learning_rate": 0.00014213114754098362,
      "loss": 0.0006,
      "step": 1060
    },
    {
      "epoch": 0.8729508196721312,
      "grad_norm": 0.000164223529282026,
      "learning_rate": 0.00014185792349726776,
      "loss": 0.0008,
      "step": 1065
    },
    {
      "epoch": 0.8770491803278688,
      "grad_norm": 0.02034379169344902,
      "learning_rate": 0.00014158469945355193,
      "loss": 0.0005,
      "step": 1070
    },
    {
      "epoch": 0.8811475409836066,
      "grad_norm": 0.12764950096607208,
      "learning_rate": 0.00014131147540983607,
      "loss": 0.0009,
      "step": 1075
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 0.003921673633158207,
      "learning_rate": 0.00014103825136612023,
      "loss": 0.0004,
      "step": 1080
    },
    {
      "epoch": 0.889344262295082,
      "grad_norm": 0.0036915477830916643,
      "learning_rate": 0.00014076502732240437,
      "loss": 0.0004,
      "step": 1085
    },
    {
      "epoch": 0.8934426229508197,
      "grad_norm": 0.009272467344999313,
      "learning_rate": 0.00014049180327868854,
      "loss": 0.0002,
      "step": 1090
    },
    {
      "epoch": 0.8975409836065574,
      "grad_norm": 0.0013436581939458847,
      "learning_rate": 0.00014021857923497268,
      "loss": 0.0005,
      "step": 1095
    },
    {
      "epoch": 0.9016393442622951,
      "grad_norm": 0.00018279057985637337,
      "learning_rate": 0.00013994535519125685,
      "loss": 0.0001,
      "step": 1100
    },
    {
      "epoch": 0.9057377049180327,
      "grad_norm": 0.0003492105461191386,
      "learning_rate": 0.000139672131147541,
      "loss": 0.0002,
      "step": 1105
    },
    {
      "epoch": 0.9098360655737705,
      "grad_norm": 0.01673228107392788,
      "learning_rate": 0.00013939890710382516,
      "loss": 0.0009,
      "step": 1110
    },
    {
      "epoch": 0.9139344262295082,
      "grad_norm": 0.0019281478598713875,
      "learning_rate": 0.0001391256830601093,
      "loss": 0.0004,
      "step": 1115
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 0.006077837198972702,
      "learning_rate": 0.00013885245901639346,
      "loss": 0.0002,
      "step": 1120
    },
    {
      "epoch": 0.9221311475409836,
      "grad_norm": 0.0013992706080898643,
      "learning_rate": 0.0001385792349726776,
      "loss": 0.0003,
      "step": 1125
    },
    {
      "epoch": 0.9262295081967213,
      "grad_norm": 0.0016170593444257975,
      "learning_rate": 0.00013830601092896177,
      "loss": 0.0005,
      "step": 1130
    },
    {
      "epoch": 0.930327868852459,
      "grad_norm": 0.0010380421299487352,
      "learning_rate": 0.0001380327868852459,
      "loss": 0.0002,
      "step": 1135
    },
    {
      "epoch": 0.9344262295081968,
      "grad_norm": 0.006002561654895544,
      "learning_rate": 0.00013775956284153008,
      "loss": 0.0021,
      "step": 1140
    },
    {
      "epoch": 0.9385245901639344,
      "grad_norm": 0.0030343758407980204,
      "learning_rate": 0.00013748633879781422,
      "loss": 0.0004,
      "step": 1145
    },
    {
      "epoch": 0.9426229508196722,
      "grad_norm": 0.06433916836977005,
      "learning_rate": 0.00013721311475409838,
      "loss": 0.0007,
      "step": 1150
    },
    {
      "epoch": 0.9467213114754098,
      "grad_norm": 0.001449406030587852,
      "learning_rate": 0.00013693989071038252,
      "loss": 0.0002,
      "step": 1155
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 0.00058654451277107,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.0002,
      "step": 1160
    },
    {
      "epoch": 0.9549180327868853,
      "grad_norm": 0.0018500920850783587,
      "learning_rate": 0.00013639344262295083,
      "loss": 0.0002,
      "step": 1165
    },
    {
      "epoch": 0.9590163934426229,
      "grad_norm": 0.0007623726851306856,
      "learning_rate": 0.00013612021857923497,
      "loss": 0.0007,
      "step": 1170
    },
    {
      "epoch": 0.9631147540983607,
      "grad_norm": 0.0048635173588991165,
      "learning_rate": 0.0001358469945355191,
      "loss": 0.0004,
      "step": 1175
    },
    {
      "epoch": 0.9672131147540983,
      "grad_norm": 0.0033725278917700052,
      "learning_rate": 0.00013557377049180328,
      "loss": 0.0004,
      "step": 1180
    },
    {
      "epoch": 0.9713114754098361,
      "grad_norm": 0.0025824429467320442,
      "learning_rate": 0.00013530054644808742,
      "loss": 0.0006,
      "step": 1185
    },
    {
      "epoch": 0.9754098360655737,
      "grad_norm": 0.01809200458228588,
      "learning_rate": 0.00013502732240437159,
      "loss": 0.0006,
      "step": 1190
    },
    {
      "epoch": 0.9795081967213115,
      "grad_norm": 0.0017302167834714055,
      "learning_rate": 0.00013475409836065573,
      "loss": 0.0005,
      "step": 1195
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 0.00015758209337946028,
      "learning_rate": 0.0001344808743169399,
      "loss": 0.0004,
      "step": 1200
    },
    {
      "epoch": 0.9877049180327869,
      "grad_norm": 0.001532477792352438,
      "learning_rate": 0.00013420765027322403,
      "loss": 0.0001,
      "step": 1205
    },
    {
      "epoch": 0.9918032786885246,
      "grad_norm": 0.0008180724689736962,
      "learning_rate": 0.0001339344262295082,
      "loss": 0.0003,
      "step": 1210
    },
    {
      "epoch": 0.9959016393442623,
      "grad_norm": 0.000277614570222795,
      "learning_rate": 0.00013366120218579237,
      "loss": 0.0002,
      "step": 1215
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.002776428824290633,
      "learning_rate": 0.0001333879781420765,
      "loss": 0.0002,
      "step": 1220
    },
    {
      "epoch": 1.0040983606557377,
      "grad_norm": 0.02655249461531639,
      "learning_rate": 0.00013311475409836067,
      "loss": 0.0004,
      "step": 1225
    },
    {
      "epoch": 1.0081967213114753,
      "grad_norm": 0.0006007161573506892,
      "learning_rate": 0.0001328415300546448,
      "loss": 0.0001,
      "step": 1230
    },
    {
      "epoch": 1.0122950819672132,
      "grad_norm": 0.029652189463377,
      "learning_rate": 0.00013256830601092898,
      "loss": 0.0005,
      "step": 1235
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 0.0017862051026895642,
      "learning_rate": 0.00013229508196721312,
      "loss": 0.0001,
      "step": 1240
    },
    {
      "epoch": 1.0204918032786885,
      "grad_norm": 2.598354512883816e-05,
      "learning_rate": 0.0001320218579234973,
      "loss": 0.0009,
      "step": 1245
    },
    {
      "epoch": 1.0245901639344261,
      "grad_norm": 0.00035268397186882794,
      "learning_rate": 0.00013174863387978143,
      "loss": 0.0007,
      "step": 1250
    },
    {
      "epoch": 1.028688524590164,
      "grad_norm": 0.003662915900349617,
      "learning_rate": 0.0001314754098360656,
      "loss": 0.0002,
      "step": 1255
    },
    {
      "epoch": 1.0327868852459017,
      "grad_norm": 0.002379933139309287,
      "learning_rate": 0.00013120218579234973,
      "loss": 0.0004,
      "step": 1260
    },
    {
      "epoch": 1.0368852459016393,
      "grad_norm": 0.006878622341901064,
      "learning_rate": 0.0001309289617486339,
      "loss": 0.0002,
      "step": 1265
    },
    {
      "epoch": 1.040983606557377,
      "grad_norm": 0.0008950517512857914,
      "learning_rate": 0.00013065573770491804,
      "loss": 0.0003,
      "step": 1270
    },
    {
      "epoch": 1.0450819672131149,
      "grad_norm": 0.0007496739272028208,
      "learning_rate": 0.0001303825136612022,
      "loss": 0.0011,
      "step": 1275
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 0.0030558574944734573,
      "learning_rate": 0.00013010928961748635,
      "loss": 0.0002,
      "step": 1280
    },
    {
      "epoch": 1.0532786885245902,
      "grad_norm": 0.0005652621039189398,
      "learning_rate": 0.00012983606557377052,
      "loss": 0.0001,
      "step": 1285
    },
    {
      "epoch": 1.0573770491803278,
      "grad_norm": 0.00020617105474229902,
      "learning_rate": 0.00012956284153005466,
      "loss": 0.0002,
      "step": 1290
    },
    {
      "epoch": 1.0614754098360655,
      "grad_norm": 0.0011703720083460212,
      "learning_rate": 0.00012928961748633882,
      "loss": 0.0009,
      "step": 1295
    },
    {
      "epoch": 1.0655737704918034,
      "grad_norm": 0.00044316850835457444,
      "learning_rate": 0.00012901639344262296,
      "loss": 0.0001,
      "step": 1300
    },
    {
      "epoch": 1.069672131147541,
      "grad_norm": 0.00299637159332633,
      "learning_rate": 0.0001287431693989071,
      "loss": 0.0003,
      "step": 1305
    },
    {
      "epoch": 1.0737704918032787,
      "grad_norm": 0.0004825017531402409,
      "learning_rate": 0.00012846994535519127,
      "loss": 0.0002,
      "step": 1310
    },
    {
      "epoch": 1.0778688524590163,
      "grad_norm": 0.009396213106811047,
      "learning_rate": 0.0001281967213114754,
      "loss": 0.0002,
      "step": 1315
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 0.0004743629542645067,
      "learning_rate": 0.00012792349726775955,
      "loss": 0.0005,
      "step": 1320
    },
    {
      "epoch": 1.0860655737704918,
      "grad_norm": 0.007257436402142048,
      "learning_rate": 0.00012765027322404372,
      "loss": 0.0003,
      "step": 1325
    },
    {
      "epoch": 1.0901639344262295,
      "grad_norm": 0.009631735272705555,
      "learning_rate": 0.00012737704918032786,
      "loss": 0.0004,
      "step": 1330
    },
    {
      "epoch": 1.0942622950819672,
      "grad_norm": 0.006310819182544947,
      "learning_rate": 0.00012710382513661202,
      "loss": 0.0003,
      "step": 1335
    },
    {
      "epoch": 1.098360655737705,
      "grad_norm": 0.00030461454298347235,
      "learning_rate": 0.00012683060109289616,
      "loss": 0.0002,
      "step": 1340
    },
    {
      "epoch": 1.1024590163934427,
      "grad_norm": 0.003052534768357873,
      "learning_rate": 0.00012655737704918033,
      "loss": 0.0002,
      "step": 1345
    },
    {
      "epoch": 1.1065573770491803,
      "grad_norm": 0.011136494576931,
      "learning_rate": 0.00012628415300546447,
      "loss": 0.0008,
      "step": 1350
    },
    {
      "epoch": 1.110655737704918,
      "grad_norm": 0.0015124179190024734,
      "learning_rate": 0.00012601092896174864,
      "loss": 0.0002,
      "step": 1355
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 0.00042045448208227754,
      "learning_rate": 0.00012573770491803278,
      "loss": 0.0001,
      "step": 1360
    },
    {
      "epoch": 1.1188524590163935,
      "grad_norm": 0.0016602784162387252,
      "learning_rate": 0.00012546448087431695,
      "loss": 0.0002,
      "step": 1365
    },
    {
      "epoch": 1.1229508196721312,
      "grad_norm": 7.617013034177944e-05,
      "learning_rate": 0.00012519125683060109,
      "loss": 0.0003,
      "step": 1370
    },
    {
      "epoch": 1.1270491803278688,
      "grad_norm": 1.3596145436167717e-05,
      "learning_rate": 0.00012491803278688525,
      "loss": 0.0001,
      "step": 1375
    },
    {
      "epoch": 1.1311475409836065,
      "grad_norm": 0.00039830897003412247,
      "learning_rate": 0.0001246448087431694,
      "loss": 0.0001,
      "step": 1380
    },
    {
      "epoch": 1.1352459016393444,
      "grad_norm": 0.004205585923045874,
      "learning_rate": 0.00012437158469945356,
      "loss": 0.0002,
      "step": 1385
    },
    {
      "epoch": 1.139344262295082,
      "grad_norm": 0.0023438837379217148,
      "learning_rate": 0.0001240983606557377,
      "loss": 0.0003,
      "step": 1390
    },
    {
      "epoch": 1.1434426229508197,
      "grad_norm": 0.0018822313286364079,
      "learning_rate": 0.00012382513661202187,
      "loss": 0.0001,
      "step": 1395
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 0.0025507837999612093,
      "learning_rate": 0.00012355191256830603,
      "loss": 0.0003,
      "step": 1400
    },
    {
      "epoch": 1.151639344262295,
      "grad_norm": 0.001969727221876383,
      "learning_rate": 0.00012327868852459017,
      "loss": 0.0002,
      "step": 1405
    },
    {
      "epoch": 1.1557377049180328,
      "grad_norm": 0.004126941319555044,
      "learning_rate": 0.00012300546448087434,
      "loss": 0.0002,
      "step": 1410
    },
    {
      "epoch": 1.1598360655737705,
      "grad_norm": 6.033002136973664e-05,
      "learning_rate": 0.00012273224043715848,
      "loss": 0.0003,
      "step": 1415
    },
    {
      "epoch": 1.1639344262295082,
      "grad_norm": 0.0012537690345197916,
      "learning_rate": 0.00012245901639344265,
      "loss": 0.0002,
      "step": 1420
    },
    {
      "epoch": 1.1680327868852458,
      "grad_norm": 0.0018491378286853433,
      "learning_rate": 0.0001221857923497268,
      "loss": 0.0002,
      "step": 1425
    },
    {
      "epoch": 1.1721311475409837,
      "grad_norm": 0.02502857707440853,
      "learning_rate": 0.00012191256830601094,
      "loss": 0.0003,
      "step": 1430
    },
    {
      "epoch": 1.1762295081967213,
      "grad_norm": 0.011267426423728466,
      "learning_rate": 0.00012163934426229508,
      "loss": 0.0003,
      "step": 1435
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 0.0023777021560817957,
      "learning_rate": 0.00012136612021857925,
      "loss": 0.0002,
      "step": 1440
    },
    {
      "epoch": 1.1844262295081966,
      "grad_norm": 0.0015517434803768992,
      "learning_rate": 0.00012109289617486339,
      "loss": 0.0003,
      "step": 1445
    },
    {
      "epoch": 1.1885245901639343,
      "grad_norm": 3.5332664992893115e-05,
      "learning_rate": 0.00012081967213114756,
      "loss": 0.0003,
      "step": 1450
    },
    {
      "epoch": 1.1926229508196722,
      "grad_norm": 0.0004914128221571445,
      "learning_rate": 0.0001205464480874317,
      "loss": 0.0003,
      "step": 1455
    },
    {
      "epoch": 1.1967213114754098,
      "grad_norm": 0.004731389228254557,
      "learning_rate": 0.00012027322404371586,
      "loss": 0.0004,
      "step": 1460
    },
    {
      "epoch": 1.2008196721311475,
      "grad_norm": 0.0003742514527402818,
      "learning_rate": 0.00012,
      "loss": 0.0002,
      "step": 1465
    },
    {
      "epoch": 1.2049180327868854,
      "grad_norm": 0.00461757043376565,
      "learning_rate": 0.00011972677595628417,
      "loss": 0.0002,
      "step": 1470
    },
    {
      "epoch": 1.209016393442623,
      "grad_norm": 2.0190256691421382e-05,
      "learning_rate": 0.00011945355191256831,
      "loss": 0.0003,
      "step": 1475
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 0.007319913245737553,
      "learning_rate": 0.00011918032786885248,
      "loss": 0.0239,
      "step": 1480
    },
    {
      "epoch": 1.2172131147540983,
      "grad_norm": 0.0005722435307689011,
      "learning_rate": 0.00011890710382513662,
      "loss": 0.0001,
      "step": 1485
    },
    {
      "epoch": 1.221311475409836,
      "grad_norm": 0.0010803983313962817,
      "learning_rate": 0.00011863387978142077,
      "loss": 0.0001,
      "step": 1490
    },
    {
      "epoch": 1.2254098360655739,
      "grad_norm": 0.00052156817400828,
      "learning_rate": 0.00011836065573770491,
      "loss": 0.0005,
      "step": 1495
    },
    {
      "epoch": 1.2295081967213115,
      "grad_norm": 0.00038197298999875784,
      "learning_rate": 0.00011808743169398908,
      "loss": 0.0003,
      "step": 1500
    },
    {
      "epoch": 1.2336065573770492,
      "grad_norm": 0.006443175021559,
      "learning_rate": 0.00011781420765027322,
      "loss": 0.0002,
      "step": 1505
    },
    {
      "epoch": 1.2377049180327868,
      "grad_norm": 8.365989924641326e-05,
      "learning_rate": 0.00011754098360655738,
      "loss": 0.0003,
      "step": 1510
    },
    {
      "epoch": 1.2418032786885247,
      "grad_norm": 0.0018709534779191017,
      "learning_rate": 0.00011726775956284152,
      "loss": 0.0002,
      "step": 1515
    },
    {
      "epoch": 1.2459016393442623,
      "grad_norm": 2.8071510769223096e-06,
      "learning_rate": 0.00011699453551912569,
      "loss": 0.0001,
      "step": 1520
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.025967363268136978,
      "learning_rate": 0.00011672131147540983,
      "loss": 0.0003,
      "step": 1525
    },
    {
      "epoch": 1.2540983606557377,
      "grad_norm": 0.0005553117953240871,
      "learning_rate": 0.000116448087431694,
      "loss": 0.0004,
      "step": 1530
    },
    {
      "epoch": 1.2581967213114753,
      "grad_norm": 0.007313434965908527,
      "learning_rate": 0.00011617486338797814,
      "loss": 0.0002,
      "step": 1535
    },
    {
      "epoch": 1.2622950819672132,
      "grad_norm": 8.240251190727577e-05,
      "learning_rate": 0.0001159016393442623,
      "loss": 0.0002,
      "step": 1540
    },
    {
      "epoch": 1.2663934426229508,
      "grad_norm": 0.0020231299567967653,
      "learning_rate": 0.00011562841530054645,
      "loss": 0.0005,
      "step": 1545
    },
    {
      "epoch": 1.2704918032786885,
      "grad_norm": 0.002674564253538847,
      "learning_rate": 0.00011535519125683061,
      "loss": 0.0006,
      "step": 1550
    },
    {
      "epoch": 1.2745901639344264,
      "grad_norm": 0.0011230248492211103,
      "learning_rate": 0.00011508196721311475,
      "loss": 0.0003,
      "step": 1555
    },
    {
      "epoch": 1.278688524590164,
      "grad_norm": 0.003594662295654416,
      "learning_rate": 0.00011480874316939891,
      "loss": 0.0003,
      "step": 1560
    },
    {
      "epoch": 1.2827868852459017,
      "grad_norm": 0.003535849042236805,
      "learning_rate": 0.00011453551912568306,
      "loss": 0.0002,
      "step": 1565
    },
    {
      "epoch": 1.2868852459016393,
      "grad_norm": 0.0026822283398360014,
      "learning_rate": 0.00011426229508196721,
      "loss": 0.0162,
      "step": 1570
    },
    {
      "epoch": 1.290983606557377,
      "grad_norm": 0.0011127227917313576,
      "learning_rate": 0.00011398907103825138,
      "loss": 0.0011,
      "step": 1575
    },
    {
      "epoch": 1.2950819672131146,
      "grad_norm": 0.012559326365590096,
      "learning_rate": 0.00011371584699453552,
      "loss": 0.0003,
      "step": 1580
    },
    {
      "epoch": 1.2991803278688525,
      "grad_norm": 0.19194786250591278,
      "learning_rate": 0.00011344262295081969,
      "loss": 0.0008,
      "step": 1585
    },
    {
      "epoch": 1.3032786885245902,
      "grad_norm": 0.004987652413547039,
      "learning_rate": 0.00011316939890710383,
      "loss": 0.0002,
      "step": 1590
    },
    {
      "epoch": 1.3073770491803278,
      "grad_norm": 0.0015526177594438195,
      "learning_rate": 0.000112896174863388,
      "loss": 0.0003,
      "step": 1595
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 0.0027950857765972614,
      "learning_rate": 0.00011262295081967214,
      "loss": 0.0002,
      "step": 1600
    },
    {
      "epoch": 1.3155737704918034,
      "grad_norm": 0.0012693729950115085,
      "learning_rate": 0.0001123497267759563,
      "loss": 0.0002,
      "step": 1605
    },
    {
      "epoch": 1.319672131147541,
      "grad_norm": 0.0009907984640449286,
      "learning_rate": 0.00011207650273224044,
      "loss": 0.0001,
      "step": 1610
    },
    {
      "epoch": 1.3237704918032787,
      "grad_norm": 0.0016775992698967457,
      "learning_rate": 0.00011180327868852461,
      "loss": 0.0001,
      "step": 1615
    },
    {
      "epoch": 1.3278688524590163,
      "grad_norm": 0.14958596229553223,
      "learning_rate": 0.00011153005464480875,
      "loss": 0.0013,
      "step": 1620
    },
    {
      "epoch": 1.331967213114754,
      "grad_norm": 0.0034277397207915783,
      "learning_rate": 0.0001112568306010929,
      "loss": 0.0002,
      "step": 1625
    },
    {
      "epoch": 1.3360655737704918,
      "grad_norm": 4.853596692555584e-06,
      "learning_rate": 0.00011098360655737706,
      "loss": 0.0004,
      "step": 1630
    },
    {
      "epoch": 1.3401639344262295,
      "grad_norm": 0.0009430182981304824,
      "learning_rate": 0.00011071038251366121,
      "loss": 0.0002,
      "step": 1635
    },
    {
      "epoch": 1.3442622950819672,
      "grad_norm": 0.0007883047801442444,
      "learning_rate": 0.00011043715846994535,
      "loss": 0.0002,
      "step": 1640
    },
    {
      "epoch": 1.348360655737705,
      "grad_norm": 0.002854648744687438,
      "learning_rate": 0.00011016393442622952,
      "loss": 0.0001,
      "step": 1645
    },
    {
      "epoch": 1.3524590163934427,
      "grad_norm": 9.150386176770553e-05,
      "learning_rate": 0.00010989071038251366,
      "loss": 0.0002,
      "step": 1650
    },
    {
      "epoch": 1.3565573770491803,
      "grad_norm": 0.0012605556985363364,
      "learning_rate": 0.00010961748633879782,
      "loss": 0.0006,
      "step": 1655
    },
    {
      "epoch": 1.360655737704918,
      "grad_norm": 0.0007745657931081951,
      "learning_rate": 0.00010934426229508196,
      "loss": 0.0003,
      "step": 1660
    },
    {
      "epoch": 1.3647540983606556,
      "grad_norm": 0.0059701865538954735,
      "learning_rate": 0.00010907103825136613,
      "loss": 0.0005,
      "step": 1665
    },
    {
      "epoch": 1.3688524590163935,
      "grad_norm": 0.0008643334731459618,
      "learning_rate": 0.00010879781420765027,
      "loss": 0.0003,
      "step": 1670
    },
    {
      "epoch": 1.3729508196721312,
      "grad_norm": 0.0006184272351674736,
      "learning_rate": 0.00010852459016393444,
      "loss": 0.0003,
      "step": 1675
    },
    {
      "epoch": 1.3770491803278688,
      "grad_norm": 0.0018317929934710264,
      "learning_rate": 0.00010825136612021858,
      "loss": 0.0002,
      "step": 1680
    },
    {
      "epoch": 1.3811475409836065,
      "grad_norm": 0.00011601027654251084,
      "learning_rate": 0.00010797814207650275,
      "loss": 0.0009,
      "step": 1685
    },
    {
      "epoch": 1.3852459016393444,
      "grad_norm": 0.0010786450002342463,
      "learning_rate": 0.00010770491803278689,
      "loss": 0.0001,
      "step": 1690
    },
    {
      "epoch": 1.389344262295082,
      "grad_norm": 0.0006539647001773119,
      "learning_rate": 0.00010743169398907105,
      "loss": 0.0002,
      "step": 1695
    },
    {
      "epoch": 1.3934426229508197,
      "grad_norm": 0.0010716678807511926,
      "learning_rate": 0.00010715846994535519,
      "loss": 0.0001,
      "step": 1700
    },
    {
      "epoch": 1.3975409836065573,
      "grad_norm": 0.0027889893390238285,
      "learning_rate": 0.00010688524590163935,
      "loss": 0.0003,
      "step": 1705
    },
    {
      "epoch": 1.401639344262295,
      "grad_norm": 0.005796652287244797,
      "learning_rate": 0.0001066120218579235,
      "loss": 0.0004,
      "step": 1710
    },
    {
      "epoch": 1.4057377049180328,
      "grad_norm": 0.00217997538857162,
      "learning_rate": 0.00010633879781420765,
      "loss": 0.0005,
      "step": 1715
    },
    {
      "epoch": 1.4098360655737705,
      "grad_norm": 0.0038773498963564634,
      "learning_rate": 0.0001060655737704918,
      "loss": 0.0001,
      "step": 1720
    },
    {
      "epoch": 1.4139344262295082,
      "grad_norm": 0.001428499585017562,
      "learning_rate": 0.00010579234972677596,
      "loss": 0.0008,
      "step": 1725
    },
    {
      "epoch": 1.418032786885246,
      "grad_norm": 0.0013936262112110853,
      "learning_rate": 0.0001055191256830601,
      "loss": 0.0002,
      "step": 1730
    },
    {
      "epoch": 1.4221311475409837,
      "grad_norm": 0.0021160023752599955,
      "learning_rate": 0.00010524590163934427,
      "loss": 0.0033,
      "step": 1735
    },
    {
      "epoch": 1.4262295081967213,
      "grad_norm": 0.000605363049544394,
      "learning_rate": 0.00010497267759562841,
      "loss": 0.0001,
      "step": 1740
    },
    {
      "epoch": 1.430327868852459,
      "grad_norm": 0.0037929313257336617,
      "learning_rate": 0.00010469945355191257,
      "loss": 0.0003,
      "step": 1745
    },
    {
      "epoch": 1.4344262295081966,
      "grad_norm": 0.00197759666480124,
      "learning_rate": 0.00010442622950819671,
      "loss": 0.0003,
      "step": 1750
    },
    {
      "epoch": 1.4385245901639343,
      "grad_norm": 3.6545901821227744e-05,
      "learning_rate": 0.00010415300546448088,
      "loss": 0.0002,
      "step": 1755
    },
    {
      "epoch": 1.4426229508196722,
      "grad_norm": 0.0006283766706474125,
      "learning_rate": 0.00010387978142076505,
      "loss": 0.0005,
      "step": 1760
    },
    {
      "epoch": 1.4467213114754098,
      "grad_norm": 0.0021998605225235224,
      "learning_rate": 0.00010360655737704919,
      "loss": 0.0003,
      "step": 1765
    },
    {
      "epoch": 1.4508196721311475,
      "grad_norm": 0.015587535686790943,
      "learning_rate": 0.00010333333333333334,
      "loss": 0.0003,
      "step": 1770
    },
    {
      "epoch": 1.4549180327868854,
      "grad_norm": 0.0006695694755762815,
      "learning_rate": 0.0001030601092896175,
      "loss": 0.0001,
      "step": 1775
    },
    {
      "epoch": 1.459016393442623,
      "grad_norm": 0.002201087074354291,
      "learning_rate": 0.00010278688524590165,
      "loss": 0.0004,
      "step": 1780
    },
    {
      "epoch": 1.4631147540983607,
      "grad_norm": 0.001831177156418562,
      "learning_rate": 0.00010251366120218579,
      "loss": 0.0008,
      "step": 1785
    },
    {
      "epoch": 1.4672131147540983,
      "grad_norm": 0.0028252892661839724,
      "learning_rate": 0.00010224043715846996,
      "loss": 0.0001,
      "step": 1790
    },
    {
      "epoch": 1.471311475409836,
      "grad_norm": 0.0008670890238136053,
      "learning_rate": 0.0001019672131147541,
      "loss": 0.0002,
      "step": 1795
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 0.0025803770404309034,
      "learning_rate": 0.00010169398907103826,
      "loss": 0.0002,
      "step": 1800
    },
    {
      "epoch": 1.4795081967213115,
      "grad_norm": 0.01846708543598652,
      "learning_rate": 0.0001014207650273224,
      "loss": 0.0003,
      "step": 1805
    },
    {
      "epoch": 1.4836065573770492,
      "grad_norm": 0.0005022974801249802,
      "learning_rate": 0.00010114754098360657,
      "loss": 0.0001,
      "step": 1810
    },
    {
      "epoch": 1.4877049180327868,
      "grad_norm": 0.001168440910987556,
      "learning_rate": 0.00010087431693989071,
      "loss": 0.0001,
      "step": 1815
    },
    {
      "epoch": 1.4918032786885247,
      "grad_norm": 0.00012711035378742963,
      "learning_rate": 0.00010060109289617488,
      "loss": 0.0002,
      "step": 1820
    },
    {
      "epoch": 1.4959016393442623,
      "grad_norm": 0.00030176943982951343,
      "learning_rate": 0.00010032786885245902,
      "loss": 0.0002,
      "step": 1825
    },
    {
      "epoch": 1.5,
      "grad_norm": 6.090498936828226e-05,
      "learning_rate": 0.00010005464480874318,
      "loss": 0.0291,
      "step": 1830
    },
    {
      "epoch": 1.5040983606557377,
      "grad_norm": 0.003090218873694539,
      "learning_rate": 9.978142076502732e-05,
      "loss": 0.0004,
      "step": 1835
    },
    {
      "epoch": 1.5081967213114753,
      "grad_norm": 0.0012928054202347994,
      "learning_rate": 9.950819672131148e-05,
      "loss": 0.0001,
      "step": 1840
    },
    {
      "epoch": 1.512295081967213,
      "grad_norm": 0.01168960053473711,
      "learning_rate": 9.923497267759563e-05,
      "loss": 0.0005,
      "step": 1845
    },
    {
      "epoch": 1.5163934426229508,
      "grad_norm": 0.000858725281432271,
      "learning_rate": 9.896174863387979e-05,
      "loss": 0.0002,
      "step": 1850
    },
    {
      "epoch": 1.5204918032786885,
      "grad_norm": 0.0011582400184124708,
      "learning_rate": 9.868852459016394e-05,
      "loss": 0.0004,
      "step": 1855
    },
    {
      "epoch": 1.5245901639344264,
      "grad_norm": 0.0007002103375270963,
      "learning_rate": 9.841530054644809e-05,
      "loss": 0.0001,
      "step": 1860
    },
    {
      "epoch": 1.528688524590164,
      "grad_norm": 0.020823884755373,
      "learning_rate": 9.814207650273225e-05,
      "loss": 0.0004,
      "step": 1865
    },
    {
      "epoch": 1.5327868852459017,
      "grad_norm": 0.005378182046115398,
      "learning_rate": 9.78688524590164e-05,
      "loss": 0.0002,
      "step": 1870
    },
    {
      "epoch": 1.5368852459016393,
      "grad_norm": 0.0016673584468662739,
      "learning_rate": 9.759562841530055e-05,
      "loss": 0.0003,
      "step": 1875
    },
    {
      "epoch": 1.540983606557377,
      "grad_norm": 0.0007428504177369177,
      "learning_rate": 9.732240437158471e-05,
      "loss": 0.0004,
      "step": 1880
    },
    {
      "epoch": 1.5450819672131146,
      "grad_norm": 0.0015767019940540195,
      "learning_rate": 9.704918032786886e-05,
      "loss": 0.0002,
      "step": 1885
    },
    {
      "epoch": 1.5491803278688525,
      "grad_norm": 0.003217651043087244,
      "learning_rate": 9.677595628415301e-05,
      "loss": 0.0003,
      "step": 1890
    },
    {
      "epoch": 1.5532786885245902,
      "grad_norm": 1.7687067156657577e-05,
      "learning_rate": 9.650273224043717e-05,
      "loss": 0.0002,
      "step": 1895
    },
    {
      "epoch": 1.5573770491803278,
      "grad_norm": 0.016950806602835655,
      "learning_rate": 9.622950819672132e-05,
      "loss": 0.0004,
      "step": 1900
    },
    {
      "epoch": 1.5614754098360657,
      "grad_norm": 0.0004561755631584674,
      "learning_rate": 9.595628415300547e-05,
      "loss": 0.0004,
      "step": 1905
    },
    {
      "epoch": 1.5655737704918034,
      "grad_norm": 0.0016303814481943846,
      "learning_rate": 9.568306010928963e-05,
      "loss": 0.0002,
      "step": 1910
    },
    {
      "epoch": 1.569672131147541,
      "grad_norm": 0.00046296321670524776,
      "learning_rate": 9.540983606557378e-05,
      "loss": 0.0003,
      "step": 1915
    },
    {
      "epoch": 1.5737704918032787,
      "grad_norm": 0.0036552955862134695,
      "learning_rate": 9.513661202185794e-05,
      "loss": 0.0003,
      "step": 1920
    },
    {
      "epoch": 1.5778688524590163,
      "grad_norm": 0.00647338991984725,
      "learning_rate": 9.486338797814209e-05,
      "loss": 0.0004,
      "step": 1925
    },
    {
      "epoch": 1.581967213114754,
      "grad_norm": 0.013110799714922905,
      "learning_rate": 9.459016393442623e-05,
      "loss": 0.0007,
      "step": 1930
    },
    {
      "epoch": 1.5860655737704918,
      "grad_norm": 0.005956871435046196,
      "learning_rate": 9.431693989071038e-05,
      "loss": 0.0007,
      "step": 1935
    },
    {
      "epoch": 1.5901639344262295,
      "grad_norm": 0.0276838019490242,
      "learning_rate": 9.404371584699454e-05,
      "loss": 0.0003,
      "step": 1940
    },
    {
      "epoch": 1.5942622950819674,
      "grad_norm": 0.3374437987804413,
      "learning_rate": 9.377049180327869e-05,
      "loss": 0.0018,
      "step": 1945
    },
    {
      "epoch": 1.598360655737705,
      "grad_norm": 0.0005227961228229105,
      "learning_rate": 9.349726775956284e-05,
      "loss": 0.0001,
      "step": 1950
    },
    {
      "epoch": 1.6024590163934427,
      "grad_norm": 0.00420130230486393,
      "learning_rate": 9.3224043715847e-05,
      "loss": 0.0003,
      "step": 1955
    },
    {
      "epoch": 1.6065573770491803,
      "grad_norm": 0.007304637227207422,
      "learning_rate": 9.295081967213115e-05,
      "loss": 0.0003,
      "step": 1960
    },
    {
      "epoch": 1.610655737704918,
      "grad_norm": 0.0010182055411860347,
      "learning_rate": 9.26775956284153e-05,
      "loss": 0.0001,
      "step": 1965
    },
    {
      "epoch": 1.6147540983606556,
      "grad_norm": 0.029651721939444542,
      "learning_rate": 9.240437158469946e-05,
      "loss": 0.0002,
      "step": 1970
    },
    {
      "epoch": 1.6188524590163933,
      "grad_norm": 0.00048013823106884956,
      "learning_rate": 9.213114754098361e-05,
      "loss": 0.0002,
      "step": 1975
    },
    {
      "epoch": 1.6229508196721312,
      "grad_norm": 0.0004378777521196753,
      "learning_rate": 9.185792349726776e-05,
      "loss": 0.0002,
      "step": 1980
    },
    {
      "epoch": 1.6270491803278688,
      "grad_norm": 0.00018981606990564615,
      "learning_rate": 9.158469945355192e-05,
      "loss": 0.0001,
      "step": 1985
    },
    {
      "epoch": 1.6311475409836067,
      "grad_norm": 0.00029551907209679484,
      "learning_rate": 9.131147540983607e-05,
      "loss": 0.0008,
      "step": 1990
    },
    {
      "epoch": 1.6352459016393444,
      "grad_norm": 0.002404948929324746,
      "learning_rate": 9.103825136612022e-05,
      "loss": 0.0002,
      "step": 1995
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 0.0010244339937344193,
      "learning_rate": 9.076502732240438e-05,
      "loss": 0.0001,
      "step": 2000
    },
    {
      "epoch": 1.6434426229508197,
      "grad_norm": 0.003282262245193124,
      "learning_rate": 9.049180327868852e-05,
      "loss": 0.0003,
      "step": 2005
    },
    {
      "epoch": 1.6475409836065573,
      "grad_norm": 0.0054809050634503365,
      "learning_rate": 9.021857923497267e-05,
      "loss": 0.0004,
      "step": 2010
    },
    {
      "epoch": 1.651639344262295,
      "grad_norm": 0.0011726422235369682,
      "learning_rate": 8.994535519125683e-05,
      "loss": 0.0002,
      "step": 2015
    },
    {
      "epoch": 1.6557377049180326,
      "grad_norm": 0.007361507974565029,
      "learning_rate": 8.967213114754098e-05,
      "loss": 0.0003,
      "step": 2020
    },
    {
      "epoch": 1.6598360655737705,
      "grad_norm": 0.006768485531210899,
      "learning_rate": 8.939890710382513e-05,
      "loss": 0.0002,
      "step": 2025
    },
    {
      "epoch": 1.6639344262295082,
      "grad_norm": 0.0009955753339454532,
      "learning_rate": 8.912568306010929e-05,
      "loss": 0.0002,
      "step": 2030
    },
    {
      "epoch": 1.668032786885246,
      "grad_norm": 0.0004202951386105269,
      "learning_rate": 8.885245901639345e-05,
      "loss": 0.0002,
      "step": 2035
    },
    {
      "epoch": 1.6721311475409837,
      "grad_norm": 0.00536133861169219,
      "learning_rate": 8.857923497267761e-05,
      "loss": 0.0002,
      "step": 2040
    },
    {
      "epoch": 1.6762295081967213,
      "grad_norm": 0.001987464027479291,
      "learning_rate": 8.830601092896176e-05,
      "loss": 0.0002,
      "step": 2045
    },
    {
      "epoch": 1.680327868852459,
      "grad_norm": 0.0012527345679700375,
      "learning_rate": 8.803278688524591e-05,
      "loss": 0.0001,
      "step": 2050
    },
    {
      "epoch": 1.6844262295081966,
      "grad_norm": 0.0014834690373390913,
      "learning_rate": 8.775956284153007e-05,
      "loss": 0.0001,
      "step": 2055
    },
    {
      "epoch": 1.6885245901639343,
      "grad_norm": 0.00045327635598368943,
      "learning_rate": 8.748633879781422e-05,
      "loss": 0.0002,
      "step": 2060
    },
    {
      "epoch": 1.6926229508196722,
      "grad_norm": 0.0007968417485244572,
      "learning_rate": 8.721311475409837e-05,
      "loss": 0.0002,
      "step": 2065
    },
    {
      "epoch": 1.6967213114754098,
      "grad_norm": 0.0016574199544265866,
      "learning_rate": 8.693989071038251e-05,
      "loss": 0.0001,
      "step": 2070
    },
    {
      "epoch": 1.7008196721311475,
      "grad_norm": 0.0018415080849081278,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.0001,
      "step": 2075
    },
    {
      "epoch": 1.7049180327868854,
      "grad_norm": 0.006196171976625919,
      "learning_rate": 8.639344262295082e-05,
      "loss": 0.0004,
      "step": 2080
    },
    {
      "epoch": 1.709016393442623,
      "grad_norm": 0.0019310564966872334,
      "learning_rate": 8.612021857923498e-05,
      "loss": 0.0005,
      "step": 2085
    },
    {
      "epoch": 1.7131147540983607,
      "grad_norm": 0.0005307503743097186,
      "learning_rate": 8.584699453551913e-05,
      "loss": 0.0002,
      "step": 2090
    },
    {
      "epoch": 1.7172131147540983,
      "grad_norm": 0.0019401948666200042,
      "learning_rate": 8.557377049180328e-05,
      "loss": 0.0047,
      "step": 2095
    },
    {
      "epoch": 1.721311475409836,
      "grad_norm": 0.0002602159802336246,
      "learning_rate": 8.530054644808744e-05,
      "loss": 0.0001,
      "step": 2100
    },
    {
      "epoch": 1.7254098360655736,
      "grad_norm": 0.0,
      "learning_rate": 8.502732240437159e-05,
      "loss": 0.0002,
      "step": 2105
    },
    {
      "epoch": 1.7295081967213115,
      "grad_norm": 0.004516357555985451,
      "learning_rate": 8.475409836065574e-05,
      "loss": 0.0003,
      "step": 2110
    },
    {
      "epoch": 1.7336065573770492,
      "grad_norm": 0.0012636411702260375,
      "learning_rate": 8.44808743169399e-05,
      "loss": 0.0002,
      "step": 2115
    },
    {
      "epoch": 1.737704918032787,
      "grad_norm": 0.014698081649839878,
      "learning_rate": 8.420765027322405e-05,
      "loss": 0.0005,
      "step": 2120
    },
    {
      "epoch": 1.7418032786885247,
      "grad_norm": 8.650411473354325e-05,
      "learning_rate": 8.39344262295082e-05,
      "loss": 0.0001,
      "step": 2125
    },
    {
      "epoch": 1.7459016393442623,
      "grad_norm": 0.007335000671446323,
      "learning_rate": 8.366120218579236e-05,
      "loss": 0.0002,
      "step": 2130
    },
    {
      "epoch": 1.75,
      "grad_norm": 4.399510942221241e-07,
      "learning_rate": 8.338797814207651e-05,
      "loss": 0.0009,
      "step": 2135
    },
    {
      "epoch": 1.7540983606557377,
      "grad_norm": 0.001764048240147531,
      "learning_rate": 8.311475409836066e-05,
      "loss": 0.0002,
      "step": 2140
    },
    {
      "epoch": 1.7581967213114753,
      "grad_norm": 0.0008858749060891569,
      "learning_rate": 8.284153005464482e-05,
      "loss": 0.0006,
      "step": 2145
    },
    {
      "epoch": 1.762295081967213,
      "grad_norm": 0.008098135702311993,
      "learning_rate": 8.256830601092896e-05,
      "loss": 0.0002,
      "step": 2150
    },
    {
      "epoch": 1.7663934426229508,
      "grad_norm": 0.0011478967498987913,
      "learning_rate": 8.229508196721311e-05,
      "loss": 0.0002,
      "step": 2155
    },
    {
      "epoch": 1.7704918032786885,
      "grad_norm": 0.008342372253537178,
      "learning_rate": 8.202185792349726e-05,
      "loss": 0.0003,
      "step": 2160
    },
    {
      "epoch": 1.7745901639344264,
      "grad_norm": 0.0,
      "learning_rate": 8.174863387978142e-05,
      "loss": 0.0012,
      "step": 2165
    },
    {
      "epoch": 1.778688524590164,
      "grad_norm": 0.0,
      "learning_rate": 8.147540983606557e-05,
      "loss": 0.0008,
      "step": 2170
    },
    {
      "epoch": 1.7827868852459017,
      "grad_norm": 0.0001030523853842169,
      "learning_rate": 8.120218579234973e-05,
      "loss": 0.0002,
      "step": 2175
    },
    {
      "epoch": 1.7868852459016393,
      "grad_norm": 0.00040547235403209925,
      "learning_rate": 8.092896174863388e-05,
      "loss": 0.0003,
      "step": 2180
    },
    {
      "epoch": 1.790983606557377,
      "grad_norm": 0.0024261910002678633,
      "learning_rate": 8.065573770491803e-05,
      "loss": 0.0055,
      "step": 2185
    },
    {
      "epoch": 1.7950819672131146,
      "grad_norm": 0.006815628614276648,
      "learning_rate": 8.038251366120219e-05,
      "loss": 0.0001,
      "step": 2190
    },
    {
      "epoch": 1.7991803278688525,
      "grad_norm": 0.00037287079612724483,
      "learning_rate": 8.010928961748634e-05,
      "loss": 0.0001,
      "step": 2195
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 0.0016567434649914503,
      "learning_rate": 7.98360655737705e-05,
      "loss": 0.0001,
      "step": 2200
    },
    {
      "epoch": 1.8073770491803278,
      "grad_norm": 9.48549568420276e-05,
      "learning_rate": 7.956284153005465e-05,
      "loss": 0.0006,
      "step": 2205
    },
    {
      "epoch": 1.8114754098360657,
      "grad_norm": 0.00041035699541680515,
      "learning_rate": 7.92896174863388e-05,
      "loss": 0.0003,
      "step": 2210
    },
    {
      "epoch": 1.8155737704918034,
      "grad_norm": 0.0009873788803815842,
      "learning_rate": 7.901639344262295e-05,
      "loss": 0.0004,
      "step": 2215
    },
    {
      "epoch": 1.819672131147541,
      "grad_norm": 0.0004424781654961407,
      "learning_rate": 7.874316939890711e-05,
      "loss": 0.0002,
      "step": 2220
    },
    {
      "epoch": 1.8237704918032787,
      "grad_norm": 0.002117718104273081,
      "learning_rate": 7.846994535519126e-05,
      "loss": 0.0001,
      "step": 2225
    },
    {
      "epoch": 1.8278688524590163,
      "grad_norm": 0.0008036543149501085,
      "learning_rate": 7.819672131147541e-05,
      "loss": 0.0019,
      "step": 2230
    },
    {
      "epoch": 1.831967213114754,
      "grad_norm": 0.0014643321046605706,
      "learning_rate": 7.792349726775957e-05,
      "loss": 0.0001,
      "step": 2235
    },
    {
      "epoch": 1.8360655737704918,
      "grad_norm": 0.0036145125050097704,
      "learning_rate": 7.765027322404372e-05,
      "loss": 0.0002,
      "step": 2240
    },
    {
      "epoch": 1.8401639344262295,
      "grad_norm": 0.007689863443374634,
      "learning_rate": 7.737704918032788e-05,
      "loss": 0.0003,
      "step": 2245
    },
    {
      "epoch": 1.8442622950819674,
      "grad_norm": 0.0004637539095710963,
      "learning_rate": 7.710382513661203e-05,
      "loss": 0.0035,
      "step": 2250
    },
    {
      "epoch": 1.848360655737705,
      "grad_norm": 0.0015821963315829635,
      "learning_rate": 7.683060109289618e-05,
      "loss": 0.0003,
      "step": 2255
    },
    {
      "epoch": 1.8524590163934427,
      "grad_norm": 0.001345692086033523,
      "learning_rate": 7.655737704918034e-05,
      "loss": 0.0006,
      "step": 2260
    },
    {
      "epoch": 1.8565573770491803,
      "grad_norm": 0.0018368338933214545,
      "learning_rate": 7.628415300546449e-05,
      "loss": 0.0002,
      "step": 2265
    },
    {
      "epoch": 1.860655737704918,
      "grad_norm": 0.0009429763304069638,
      "learning_rate": 7.601092896174864e-05,
      "loss": 0.0002,
      "step": 2270
    },
    {
      "epoch": 1.8647540983606556,
      "grad_norm": 0.0006086280918680131,
      "learning_rate": 7.57377049180328e-05,
      "loss": 0.0007,
      "step": 2275
    },
    {
      "epoch": 1.8688524590163933,
      "grad_norm": 0.0010886923409998417,
      "learning_rate": 7.546448087431695e-05,
      "loss": 0.0001,
      "step": 2280
    },
    {
      "epoch": 1.8729508196721312,
      "grad_norm": 1.8292638515049475e-06,
      "learning_rate": 7.51912568306011e-05,
      "loss": 0.0003,
      "step": 2285
    },
    {
      "epoch": 1.8770491803278688,
      "grad_norm": 0.0015974086709320545,
      "learning_rate": 7.491803278688526e-05,
      "loss": 0.0037,
      "step": 2290
    },
    {
      "epoch": 1.8811475409836067,
      "grad_norm": 0.00644654082134366,
      "learning_rate": 7.46448087431694e-05,
      "loss": 0.0004,
      "step": 2295
    },
    {
      "epoch": 1.8852459016393444,
      "grad_norm": 0.0018969490192830563,
      "learning_rate": 7.437158469945355e-05,
      "loss": 0.0003,
      "step": 2300
    },
    {
      "epoch": 1.889344262295082,
      "grad_norm": 0.0,
      "learning_rate": 7.40983606557377e-05,
      "loss": 0.0008,
      "step": 2305
    },
    {
      "epoch": 1.8934426229508197,
      "grad_norm": 0.004300393164157867,
      "learning_rate": 7.382513661202186e-05,
      "loss": 0.0002,
      "step": 2310
    },
    {
      "epoch": 1.8975409836065573,
      "grad_norm": 0.0005642312462441623,
      "learning_rate": 7.355191256830601e-05,
      "loss": 0.0001,
      "step": 2315
    },
    {
      "epoch": 1.901639344262295,
      "grad_norm": 0.0022080112248659134,
      "learning_rate": 7.327868852459016e-05,
      "loss": 0.0002,
      "step": 2320
    },
    {
      "epoch": 1.9057377049180326,
      "grad_norm": 0.0008177076815627515,
      "learning_rate": 7.300546448087432e-05,
      "loss": 0.0003,
      "step": 2325
    },
    {
      "epoch": 1.9098360655737705,
      "grad_norm": 0.00043771378113888204,
      "learning_rate": 7.273224043715847e-05,
      "loss": 0.0001,
      "step": 2330
    },
    {
      "epoch": 1.9139344262295082,
      "grad_norm": 0.015817372128367424,
      "learning_rate": 7.245901639344263e-05,
      "loss": 0.0003,
      "step": 2335
    },
    {
      "epoch": 1.918032786885246,
      "grad_norm": 0.002134431852027774,
      "learning_rate": 7.218579234972678e-05,
      "loss": 0.0003,
      "step": 2340
    },
    {
      "epoch": 1.9221311475409837,
      "grad_norm": 4.6281125833047554e-05,
      "learning_rate": 7.191256830601093e-05,
      "loss": 0.0006,
      "step": 2345
    },
    {
      "epoch": 1.9262295081967213,
      "grad_norm": 0.0024972003884613514,
      "learning_rate": 7.163934426229509e-05,
      "loss": 0.0001,
      "step": 2350
    },
    {
      "epoch": 1.930327868852459,
      "grad_norm": 0.0005347662372514606,
      "learning_rate": 7.136612021857924e-05,
      "loss": 0.0006,
      "step": 2355
    },
    {
      "epoch": 1.9344262295081966,
      "grad_norm": 0.004141008015722036,
      "learning_rate": 7.10928961748634e-05,
      "loss": 0.0001,
      "step": 2360
    },
    {
      "epoch": 1.9385245901639343,
      "grad_norm": 0.0014124374138191342,
      "learning_rate": 7.081967213114755e-05,
      "loss": 0.0002,
      "step": 2365
    },
    {
      "epoch": 1.9426229508196722,
      "grad_norm": 0.003022940596565604,
      "learning_rate": 7.054644808743169e-05,
      "loss": 0.0002,
      "step": 2370
    },
    {
      "epoch": 1.9467213114754098,
      "grad_norm": 4.412511771079153e-05,
      "learning_rate": 7.027322404371584e-05,
      "loss": 0.0002,
      "step": 2375
    },
    {
      "epoch": 1.9508196721311475,
      "grad_norm": 0.0017867431743070483,
      "learning_rate": 7e-05,
      "loss": 0.0001,
      "step": 2380
    },
    {
      "epoch": 1.9549180327868854,
      "grad_norm": 0.001444009831175208,
      "learning_rate": 6.972677595628415e-05,
      "loss": 0.0001,
      "step": 2385
    },
    {
      "epoch": 1.959016393442623,
      "grad_norm": 8.828243880998343e-05,
      "learning_rate": 6.94535519125683e-05,
      "loss": 0.0002,
      "step": 2390
    },
    {
      "epoch": 1.9631147540983607,
      "grad_norm": 0.007883242331445217,
      "learning_rate": 6.918032786885245e-05,
      "loss": 0.0004,
      "step": 2395
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 0.0014126362511888146,
      "learning_rate": 6.890710382513662e-05,
      "loss": 0.0004,
      "step": 2400
    },
    {
      "epoch": 1.971311475409836,
      "grad_norm": 0.00031012226827442646,
      "learning_rate": 6.863387978142078e-05,
      "loss": 0.0003,
      "step": 2405
    },
    {
      "epoch": 1.9754098360655736,
      "grad_norm": 0.0024588427040725946,
      "learning_rate": 6.836065573770493e-05,
      "loss": 0.0002,
      "step": 2410
    },
    {
      "epoch": 1.9795081967213115,
      "grad_norm": 0.0033919350244104862,
      "learning_rate": 6.808743169398908e-05,
      "loss": 0.0009,
      "step": 2415
    },
    {
      "epoch": 1.9836065573770492,
      "grad_norm": 0.004468330182135105,
      "learning_rate": 6.781420765027324e-05,
      "loss": 0.0002,
      "step": 2420
    },
    {
      "epoch": 1.987704918032787,
      "grad_norm": 1.6801568563096225e-05,
      "learning_rate": 6.754098360655739e-05,
      "loss": 0.0001,
      "step": 2425
    },
    {
      "epoch": 1.9918032786885247,
      "grad_norm": 0.0010020051850005984,
      "learning_rate": 6.726775956284154e-05,
      "loss": 0.0001,
      "step": 2430
    },
    {
      "epoch": 1.9959016393442623,
      "grad_norm": 0.00024799074162729084,
      "learning_rate": 6.699453551912568e-05,
      "loss": 0.0001,
      "step": 2435
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.00012535409769043326,
      "learning_rate": 6.672131147540984e-05,
      "loss": 0.0002,
      "step": 2440
    },
    {
      "epoch": 2.0040983606557377,
      "grad_norm": 4.705820538219996e-05,
      "learning_rate": 6.644808743169399e-05,
      "loss": 0.0004,
      "step": 2445
    },
    {
      "epoch": 2.0081967213114753,
      "grad_norm": 0.01469461154192686,
      "learning_rate": 6.617486338797814e-05,
      "loss": 0.0003,
      "step": 2450
    },
    {
      "epoch": 2.012295081967213,
      "grad_norm": 0.0001423698413418606,
      "learning_rate": 6.59016393442623e-05,
      "loss": 0.0005,
      "step": 2455
    },
    {
      "epoch": 2.0163934426229506,
      "grad_norm": 0.0017515281215310097,
      "learning_rate": 6.562841530054645e-05,
      "loss": 0.0001,
      "step": 2460
    },
    {
      "epoch": 2.0204918032786887,
      "grad_norm": 0.030188677832484245,
      "learning_rate": 6.53551912568306e-05,
      "loss": 0.0005,
      "step": 2465
    },
    {
      "epoch": 2.0245901639344264,
      "grad_norm": 0.0004987895372323692,
      "learning_rate": 6.508196721311476e-05,
      "loss": 0.0003,
      "step": 2470
    },
    {
      "epoch": 2.028688524590164,
      "grad_norm": 0.0015084451297298074,
      "learning_rate": 6.480874316939891e-05,
      "loss": 0.0001,
      "step": 2475
    },
    {
      "epoch": 2.0327868852459017,
      "grad_norm": 0.015576157718896866,
      "learning_rate": 6.453551912568306e-05,
      "loss": 0.0004,
      "step": 2480
    },
    {
      "epoch": 2.0368852459016393,
      "grad_norm": 0.00210099620744586,
      "learning_rate": 6.426229508196722e-05,
      "loss": 0.0002,
      "step": 2485
    },
    {
      "epoch": 2.040983606557377,
      "grad_norm": 0.00046826500329189,
      "learning_rate": 6.398907103825137e-05,
      "loss": 0.0002,
      "step": 2490
    },
    {
      "epoch": 2.0450819672131146,
      "grad_norm": 0.0017400318756699562,
      "learning_rate": 6.371584699453553e-05,
      "loss": 0.0003,
      "step": 2495
    },
    {
      "epoch": 2.0491803278688523,
      "grad_norm": 0.0032283456530421972,
      "learning_rate": 6.344262295081968e-05,
      "loss": 0.0001,
      "step": 2500
    }
  ],
  "logging_steps": 5,
  "max_steps": 3660,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 681471393454080.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
