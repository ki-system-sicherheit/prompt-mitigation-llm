{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2295081967213115,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004098360655737705,
      "grad_norm": 0.9527931213378906,
      "learning_rate": 0.00019978142076502732,
      "loss": 0.8217,
      "step": 5
    },
    {
      "epoch": 0.00819672131147541,
      "grad_norm": 0.5045624375343323,
      "learning_rate": 0.00019950819672131148,
      "loss": 0.6988,
      "step": 10
    },
    {
      "epoch": 0.012295081967213115,
      "grad_norm": 0.6213607788085938,
      "learning_rate": 0.00019923497267759562,
      "loss": 0.5741,
      "step": 15
    },
    {
      "epoch": 0.01639344262295082,
      "grad_norm": 1.107638955116272,
      "learning_rate": 0.0001989617486338798,
      "loss": 0.4477,
      "step": 20
    },
    {
      "epoch": 0.020491803278688523,
      "grad_norm": 0.4475697875022888,
      "learning_rate": 0.00019868852459016393,
      "loss": 0.2998,
      "step": 25
    },
    {
      "epoch": 0.02459016393442623,
      "grad_norm": 0.43611767888069153,
      "learning_rate": 0.0001984153005464481,
      "loss": 0.1331,
      "step": 30
    },
    {
      "epoch": 0.028688524590163935,
      "grad_norm": 0.9668943285942078,
      "learning_rate": 0.00019814207650273224,
      "loss": 0.1277,
      "step": 35
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 0.7083913683891296,
      "learning_rate": 0.0001978688524590164,
      "loss": 0.1006,
      "step": 40
    },
    {
      "epoch": 0.036885245901639344,
      "grad_norm": 0.8571495413780212,
      "learning_rate": 0.00019759562841530054,
      "loss": 0.0931,
      "step": 45
    },
    {
      "epoch": 0.040983606557377046,
      "grad_norm": 1.0128223896026611,
      "learning_rate": 0.0001973224043715847,
      "loss": 0.0423,
      "step": 50
    },
    {
      "epoch": 0.045081967213114756,
      "grad_norm": 0.185210719704628,
      "learning_rate": 0.00019704918032786885,
      "loss": 0.0145,
      "step": 55
    },
    {
      "epoch": 0.04918032786885246,
      "grad_norm": 0.2549950182437897,
      "learning_rate": 0.00019677595628415302,
      "loss": 0.0202,
      "step": 60
    },
    {
      "epoch": 0.05327868852459016,
      "grad_norm": 0.011635084636509418,
      "learning_rate": 0.00019650273224043716,
      "loss": 0.0239,
      "step": 65
    },
    {
      "epoch": 0.05737704918032787,
      "grad_norm": 0.07656217366456985,
      "learning_rate": 0.00019622950819672133,
      "loss": 0.0074,
      "step": 70
    },
    {
      "epoch": 0.06147540983606557,
      "grad_norm": 0.060497161000967026,
      "learning_rate": 0.00019595628415300547,
      "loss": 0.0276,
      "step": 75
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 0.07262999564409256,
      "learning_rate": 0.00019568306010928963,
      "loss": 0.0085,
      "step": 80
    },
    {
      "epoch": 0.06967213114754098,
      "grad_norm": 0.030240889638662338,
      "learning_rate": 0.00019540983606557377,
      "loss": 0.0083,
      "step": 85
    },
    {
      "epoch": 0.07377049180327869,
      "grad_norm": 0.04026243835687637,
      "learning_rate": 0.00019513661202185794,
      "loss": 0.0052,
      "step": 90
    },
    {
      "epoch": 0.0778688524590164,
      "grad_norm": 0.19012518227100372,
      "learning_rate": 0.00019486338797814208,
      "loss": 0.0061,
      "step": 95
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 0.16719123721122742,
      "learning_rate": 0.00019459016393442625,
      "loss": 0.0101,
      "step": 100
    },
    {
      "epoch": 0.0860655737704918,
      "grad_norm": 0.017311835661530495,
      "learning_rate": 0.0001943169398907104,
      "loss": 0.0034,
      "step": 105
    },
    {
      "epoch": 0.09016393442622951,
      "grad_norm": 0.021288778632879257,
      "learning_rate": 0.00019404371584699455,
      "loss": 0.0078,
      "step": 110
    },
    {
      "epoch": 0.0942622950819672,
      "grad_norm": 0.0617251843214035,
      "learning_rate": 0.00019377049180327872,
      "loss": 0.0026,
      "step": 115
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 0.8364461660385132,
      "learning_rate": 0.00019349726775956286,
      "loss": 0.0068,
      "step": 120
    },
    {
      "epoch": 0.10245901639344263,
      "grad_norm": 0.0058639030903577805,
      "learning_rate": 0.000193224043715847,
      "loss": 0.0041,
      "step": 125
    },
    {
      "epoch": 0.10655737704918032,
      "grad_norm": 0.04045857861638069,
      "learning_rate": 0.00019295081967213117,
      "loss": 0.0072,
      "step": 130
    },
    {
      "epoch": 0.11065573770491803,
      "grad_norm": 1.3383764028549194,
      "learning_rate": 0.0001926775956284153,
      "loss": 0.0182,
      "step": 135
    },
    {
      "epoch": 0.11475409836065574,
      "grad_norm": 0.1323436200618744,
      "learning_rate": 0.00019240437158469945,
      "loss": 0.0021,
      "step": 140
    },
    {
      "epoch": 0.11885245901639344,
      "grad_norm": 0.025622591376304626,
      "learning_rate": 0.00019213114754098362,
      "loss": 0.0046,
      "step": 145
    },
    {
      "epoch": 0.12295081967213115,
      "grad_norm": 0.010446264408528805,
      "learning_rate": 0.00019185792349726776,
      "loss": 0.0027,
      "step": 150
    },
    {
      "epoch": 0.12704918032786885,
      "grad_norm": 0.03192673996090889,
      "learning_rate": 0.00019158469945355192,
      "loss": 0.0023,
      "step": 155
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 0.009083353914320469,
      "learning_rate": 0.00019131147540983606,
      "loss": 0.0012,
      "step": 160
    },
    {
      "epoch": 0.13524590163934427,
      "grad_norm": 0.01632789522409439,
      "learning_rate": 0.00019103825136612023,
      "loss": 0.002,
      "step": 165
    },
    {
      "epoch": 0.13934426229508196,
      "grad_norm": 0.03792854771018028,
      "learning_rate": 0.00019076502732240437,
      "loss": 0.0018,
      "step": 170
    },
    {
      "epoch": 0.14344262295081966,
      "grad_norm": 0.049662161618471146,
      "learning_rate": 0.00019049180327868854,
      "loss": 0.0056,
      "step": 175
    },
    {
      "epoch": 0.14754098360655737,
      "grad_norm": 0.004366317763924599,
      "learning_rate": 0.00019021857923497268,
      "loss": 0.0045,
      "step": 180
    },
    {
      "epoch": 0.15163934426229508,
      "grad_norm": 0.0021171499975025654,
      "learning_rate": 0.00018994535519125684,
      "loss": 0.0047,
      "step": 185
    },
    {
      "epoch": 0.1557377049180328,
      "grad_norm": 0.04822837933897972,
      "learning_rate": 0.00018967213114754098,
      "loss": 0.0016,
      "step": 190
    },
    {
      "epoch": 0.1598360655737705,
      "grad_norm": 0.01301040593534708,
      "learning_rate": 0.00018939890710382515,
      "loss": 0.001,
      "step": 195
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 0.09617028385400772,
      "learning_rate": 0.0001891256830601093,
      "loss": 0.0022,
      "step": 200
    },
    {
      "epoch": 0.1680327868852459,
      "grad_norm": 0.0021836638916283846,
      "learning_rate": 0.00018885245901639346,
      "loss": 0.0015,
      "step": 205
    },
    {
      "epoch": 0.1721311475409836,
      "grad_norm": 0.017412008717656136,
      "learning_rate": 0.0001885792349726776,
      "loss": 0.0021,
      "step": 210
    },
    {
      "epoch": 0.1762295081967213,
      "grad_norm": 0.008060822263360023,
      "learning_rate": 0.00018830601092896177,
      "loss": 0.0007,
      "step": 215
    },
    {
      "epoch": 0.18032786885245902,
      "grad_norm": 0.02488826774060726,
      "learning_rate": 0.0001880327868852459,
      "loss": 0.0024,
      "step": 220
    },
    {
      "epoch": 0.18442622950819673,
      "grad_norm": 0.005369509104639292,
      "learning_rate": 0.00018775956284153007,
      "loss": 0.0017,
      "step": 225
    },
    {
      "epoch": 0.1885245901639344,
      "grad_norm": 0.02311077155172825,
      "learning_rate": 0.0001874863387978142,
      "loss": 0.001,
      "step": 230
    },
    {
      "epoch": 0.19262295081967212,
      "grad_norm": 0.018028857186436653,
      "learning_rate": 0.00018721311475409838,
      "loss": 0.0024,
      "step": 235
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 0.004582926630973816,
      "learning_rate": 0.00018693989071038252,
      "loss": 0.0015,
      "step": 240
    },
    {
      "epoch": 0.20081967213114754,
      "grad_norm": 0.01514498796314001,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.0022,
      "step": 245
    },
    {
      "epoch": 0.20491803278688525,
      "grad_norm": 0.009839089587330818,
      "learning_rate": 0.00018639344262295083,
      "loss": 0.0053,
      "step": 250
    },
    {
      "epoch": 0.20901639344262296,
      "grad_norm": 1.1162108182907104,
      "learning_rate": 0.000186120218579235,
      "loss": 0.0089,
      "step": 255
    },
    {
      "epoch": 0.21311475409836064,
      "grad_norm": 0.009776981547474861,
      "learning_rate": 0.00018584699453551913,
      "loss": 0.001,
      "step": 260
    },
    {
      "epoch": 0.21721311475409835,
      "grad_norm": 0.0063355970196425915,
      "learning_rate": 0.0001855737704918033,
      "loss": 0.0017,
      "step": 265
    },
    {
      "epoch": 0.22131147540983606,
      "grad_norm": 0.005864137317985296,
      "learning_rate": 0.00018530054644808744,
      "loss": 0.0018,
      "step": 270
    },
    {
      "epoch": 0.22540983606557377,
      "grad_norm": 0.07048726826906204,
      "learning_rate": 0.0001850273224043716,
      "loss": 0.0019,
      "step": 275
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 0.02692907676100731,
      "learning_rate": 0.00018475409836065575,
      "loss": 0.0012,
      "step": 280
    },
    {
      "epoch": 0.2336065573770492,
      "grad_norm": 0.01985354721546173,
      "learning_rate": 0.0001844808743169399,
      "loss": 0.0015,
      "step": 285
    },
    {
      "epoch": 0.23770491803278687,
      "grad_norm": 0.002445412566885352,
      "learning_rate": 0.00018420765027322405,
      "loss": 0.0012,
      "step": 290
    },
    {
      "epoch": 0.24180327868852458,
      "grad_norm": 0.0151666896417737,
      "learning_rate": 0.0001839344262295082,
      "loss": 0.0008,
      "step": 295
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 0.008400567807257175,
      "learning_rate": 0.00018366120218579236,
      "loss": 0.0022,
      "step": 300
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01348680816590786,
      "learning_rate": 0.0001833879781420765,
      "loss": 0.0038,
      "step": 305
    },
    {
      "epoch": 0.2540983606557377,
      "grad_norm": 0.008146454580128193,
      "learning_rate": 0.00018311475409836067,
      "loss": 0.0006,
      "step": 310
    },
    {
      "epoch": 0.2581967213114754,
      "grad_norm": 0.07796267420053482,
      "learning_rate": 0.0001828415300546448,
      "loss": 0.0021,
      "step": 315
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 0.1860826313495636,
      "learning_rate": 0.00018256830601092898,
      "loss": 0.0072,
      "step": 320
    },
    {
      "epoch": 0.26639344262295084,
      "grad_norm": 0.0037070682737976313,
      "learning_rate": 0.00018229508196721312,
      "loss": 0.0012,
      "step": 325
    },
    {
      "epoch": 0.27049180327868855,
      "grad_norm": 0.001117024919949472,
      "learning_rate": 0.00018202185792349728,
      "loss": 0.0004,
      "step": 330
    },
    {
      "epoch": 0.27459016393442626,
      "grad_norm": 0.1939241886138916,
      "learning_rate": 0.00018174863387978142,
      "loss": 0.0063,
      "step": 335
    },
    {
      "epoch": 0.2786885245901639,
      "grad_norm": 0.020207803696393967,
      "learning_rate": 0.0001814754098360656,
      "loss": 0.001,
      "step": 340
    },
    {
      "epoch": 0.2827868852459016,
      "grad_norm": 0.010100802406668663,
      "learning_rate": 0.00018120218579234973,
      "loss": 0.0004,
      "step": 345
    },
    {
      "epoch": 0.28688524590163933,
      "grad_norm": 0.012284435331821442,
      "learning_rate": 0.0001809289617486339,
      "loss": 0.0047,
      "step": 350
    },
    {
      "epoch": 0.29098360655737704,
      "grad_norm": 0.026365771889686584,
      "learning_rate": 0.00018065573770491804,
      "loss": 0.0011,
      "step": 355
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 0.0016114693135023117,
      "learning_rate": 0.0001803825136612022,
      "loss": 0.001,
      "step": 360
    },
    {
      "epoch": 0.29918032786885246,
      "grad_norm": 0.0032059005461633205,
      "learning_rate": 0.00018010928961748634,
      "loss": 0.0023,
      "step": 365
    },
    {
      "epoch": 0.30327868852459017,
      "grad_norm": 0.055000223219394684,
      "learning_rate": 0.0001798360655737705,
      "loss": 0.0019,
      "step": 370
    },
    {
      "epoch": 0.3073770491803279,
      "grad_norm": 0.0024290892761200666,
      "learning_rate": 0.00017956284153005465,
      "loss": 0.0009,
      "step": 375
    },
    {
      "epoch": 0.3114754098360656,
      "grad_norm": 0.016742771491408348,
      "learning_rate": 0.00017928961748633882,
      "loss": 0.0008,
      "step": 380
    },
    {
      "epoch": 0.3155737704918033,
      "grad_norm": 0.002140327822417021,
      "learning_rate": 0.00017901639344262296,
      "loss": 0.0007,
      "step": 385
    },
    {
      "epoch": 0.319672131147541,
      "grad_norm": 0.020881978794932365,
      "learning_rate": 0.00017874316939890713,
      "loss": 0.0037,
      "step": 390
    },
    {
      "epoch": 0.3237704918032787,
      "grad_norm": 0.007238979451358318,
      "learning_rate": 0.00017846994535519127,
      "loss": 0.0009,
      "step": 395
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 0.10407977551221848,
      "learning_rate": 0.00017819672131147543,
      "loss": 0.0014,
      "step": 400
    },
    {
      "epoch": 0.3319672131147541,
      "grad_norm": 0.003155569778755307,
      "learning_rate": 0.00017792349726775957,
      "loss": 0.0005,
      "step": 405
    },
    {
      "epoch": 0.3360655737704918,
      "grad_norm": 0.006420270539820194,
      "learning_rate": 0.00017765027322404374,
      "loss": 0.0005,
      "step": 410
    },
    {
      "epoch": 0.3401639344262295,
      "grad_norm": 0.0073453523218631744,
      "learning_rate": 0.00017737704918032788,
      "loss": 0.0005,
      "step": 415
    },
    {
      "epoch": 0.3442622950819672,
      "grad_norm": 0.015700800344347954,
      "learning_rate": 0.00017710382513661205,
      "loss": 0.0011,
      "step": 420
    },
    {
      "epoch": 0.3483606557377049,
      "grad_norm": 0.04038052260875702,
      "learning_rate": 0.0001768306010928962,
      "loss": 0.0012,
      "step": 425
    },
    {
      "epoch": 0.3524590163934426,
      "grad_norm": 0.0008535035303793848,
      "learning_rate": 0.00017655737704918033,
      "loss": 0.0008,
      "step": 430
    },
    {
      "epoch": 0.35655737704918034,
      "grad_norm": 0.00859803706407547,
      "learning_rate": 0.0001762841530054645,
      "loss": 0.0008,
      "step": 435
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 0.44438108801841736,
      "learning_rate": 0.00017601092896174863,
      "loss": 0.0068,
      "step": 440
    },
    {
      "epoch": 0.36475409836065575,
      "grad_norm": 0.014533805660903454,
      "learning_rate": 0.00017573770491803277,
      "loss": 0.0007,
      "step": 445
    },
    {
      "epoch": 0.36885245901639346,
      "grad_norm": 0.005178009159862995,
      "learning_rate": 0.00017546448087431694,
      "loss": 0.0006,
      "step": 450
    },
    {
      "epoch": 0.3729508196721312,
      "grad_norm": 0.04526493698358536,
      "learning_rate": 0.00017519125683060108,
      "loss": 0.0006,
      "step": 455
    },
    {
      "epoch": 0.3770491803278688,
      "grad_norm": 0.011716095730662346,
      "learning_rate": 0.00017491803278688525,
      "loss": 0.002,
      "step": 460
    },
    {
      "epoch": 0.38114754098360654,
      "grad_norm": 0.0009423488518223166,
      "learning_rate": 0.0001746448087431694,
      "loss": 0.0032,
      "step": 465
    },
    {
      "epoch": 0.38524590163934425,
      "grad_norm": 0.0077596502378582954,
      "learning_rate": 0.00017437158469945356,
      "loss": 0.0011,
      "step": 470
    },
    {
      "epoch": 0.38934426229508196,
      "grad_norm": 0.0035024548415094614,
      "learning_rate": 0.0001740983606557377,
      "loss": 0.0004,
      "step": 475
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 0.18749438226222992,
      "learning_rate": 0.00017382513661202186,
      "loss": 0.0021,
      "step": 480
    },
    {
      "epoch": 0.3975409836065574,
      "grad_norm": 0.0100320503115654,
      "learning_rate": 0.00017355191256830603,
      "loss": 0.0008,
      "step": 485
    },
    {
      "epoch": 0.4016393442622951,
      "grad_norm": 0.0029418293852359056,
      "learning_rate": 0.00017327868852459017,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 0.4057377049180328,
      "grad_norm": 0.0038733775727450848,
      "learning_rate": 0.00017300546448087434,
      "loss": 0.0011,
      "step": 495
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 0.014742834493517876,
      "learning_rate": 0.00017273224043715848,
      "loss": 0.0006,
      "step": 500
    },
    {
      "epoch": 0.4139344262295082,
      "grad_norm": 0.001188283902592957,
      "learning_rate": 0.00017245901639344264,
      "loss": 0.0004,
      "step": 505
    },
    {
      "epoch": 0.4180327868852459,
      "grad_norm": 0.007202642969787121,
      "learning_rate": 0.00017218579234972678,
      "loss": 0.0009,
      "step": 510
    },
    {
      "epoch": 0.42213114754098363,
      "grad_norm": 0.0009368335595354438,
      "learning_rate": 0.00017191256830601095,
      "loss": 0.0006,
      "step": 515
    },
    {
      "epoch": 0.4262295081967213,
      "grad_norm": 0.04385055974125862,
      "learning_rate": 0.0001716393442622951,
      "loss": 0.001,
      "step": 520
    },
    {
      "epoch": 0.430327868852459,
      "grad_norm": 0.006209665909409523,
      "learning_rate": 0.00017136612021857926,
      "loss": 0.0012,
      "step": 525
    },
    {
      "epoch": 0.4344262295081967,
      "grad_norm": 0.0004584489797707647,
      "learning_rate": 0.0001710928961748634,
      "loss": 0.0011,
      "step": 530
    },
    {
      "epoch": 0.4385245901639344,
      "grad_norm": 0.15696106851100922,
      "learning_rate": 0.00017081967213114757,
      "loss": 0.0015,
      "step": 535
    },
    {
      "epoch": 0.4426229508196721,
      "grad_norm": 0.001240383367985487,
      "learning_rate": 0.0001705464480874317,
      "loss": 0.0005,
      "step": 540
    },
    {
      "epoch": 0.44672131147540983,
      "grad_norm": 0.025476330891251564,
      "learning_rate": 0.00017027322404371587,
      "loss": 0.0007,
      "step": 545
    },
    {
      "epoch": 0.45081967213114754,
      "grad_norm": 0.002015160396695137,
      "learning_rate": 0.00017,
      "loss": 0.0006,
      "step": 550
    },
    {
      "epoch": 0.45491803278688525,
      "grad_norm": 0.015847815200686455,
      "learning_rate": 0.00016972677595628418,
      "loss": 0.0005,
      "step": 555
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 0.004574120976030827,
      "learning_rate": 0.00016945355191256832,
      "loss": 0.001,
      "step": 560
    },
    {
      "epoch": 0.46311475409836067,
      "grad_norm": 0.0032532610930502415,
      "learning_rate": 0.00016918032786885249,
      "loss": 0.0004,
      "step": 565
    },
    {
      "epoch": 0.4672131147540984,
      "grad_norm": 0.006321570836007595,
      "learning_rate": 0.00016890710382513663,
      "loss": 0.0007,
      "step": 570
    },
    {
      "epoch": 0.4713114754098361,
      "grad_norm": 0.010698975063860416,
      "learning_rate": 0.00016863387978142077,
      "loss": 0.0008,
      "step": 575
    },
    {
      "epoch": 0.47540983606557374,
      "grad_norm": 0.005457952152937651,
      "learning_rate": 0.0001683606557377049,
      "loss": 0.0005,
      "step": 580
    },
    {
      "epoch": 0.47950819672131145,
      "grad_norm": 0.0009932118700817227,
      "learning_rate": 0.00016808743169398907,
      "loss": 0.0014,
      "step": 585
    },
    {
      "epoch": 0.48360655737704916,
      "grad_norm": 0.0011652065441012383,
      "learning_rate": 0.0001678142076502732,
      "loss": 0.0094,
      "step": 590
    },
    {
      "epoch": 0.48770491803278687,
      "grad_norm": 0.001046116347424686,
      "learning_rate": 0.00016754098360655738,
      "loss": 0.0004,
      "step": 595
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 0.0011388229904696345,
      "learning_rate": 0.00016726775956284152,
      "loss": 0.0004,
      "step": 600
    },
    {
      "epoch": 0.4959016393442623,
      "grad_norm": 0.009055097587406635,
      "learning_rate": 0.0001669945355191257,
      "loss": 0.0007,
      "step": 605
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.006677847355604172,
      "learning_rate": 0.00016672131147540983,
      "loss": 0.0005,
      "step": 610
    },
    {
      "epoch": 0.5040983606557377,
      "grad_norm": 0.0036536981351673603,
      "learning_rate": 0.000166448087431694,
      "loss": 0.0011,
      "step": 615
    },
    {
      "epoch": 0.5081967213114754,
      "grad_norm": 0.01242875587195158,
      "learning_rate": 0.00016617486338797813,
      "loss": 0.0005,
      "step": 620
    },
    {
      "epoch": 0.5122950819672131,
      "grad_norm": 0.002459513023495674,
      "learning_rate": 0.0001659016393442623,
      "loss": 0.0004,
      "step": 625
    },
    {
      "epoch": 0.5163934426229508,
      "grad_norm": 0.012614401988685131,
      "learning_rate": 0.00016562841530054644,
      "loss": 0.0007,
      "step": 630
    },
    {
      "epoch": 0.5204918032786885,
      "grad_norm": 0.017447732388973236,
      "learning_rate": 0.0001653551912568306,
      "loss": 0.0007,
      "step": 635
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 0.08051905035972595,
      "learning_rate": 0.00016508196721311475,
      "loss": 0.0025,
      "step": 640
    },
    {
      "epoch": 0.5286885245901639,
      "grad_norm": 0.03263995796442032,
      "learning_rate": 0.00016480874316939892,
      "loss": 0.0007,
      "step": 645
    },
    {
      "epoch": 0.5327868852459017,
      "grad_norm": 0.0012347821611911058,
      "learning_rate": 0.00016453551912568306,
      "loss": 0.0016,
      "step": 650
    },
    {
      "epoch": 0.5368852459016393,
      "grad_norm": 0.0004190754843875766,
      "learning_rate": 0.00016426229508196722,
      "loss": 0.0009,
      "step": 655
    },
    {
      "epoch": 0.5409836065573771,
      "grad_norm": 0.0014187332708388567,
      "learning_rate": 0.0001639890710382514,
      "loss": 0.0027,
      "step": 660
    },
    {
      "epoch": 0.5450819672131147,
      "grad_norm": 0.0004880629130639136,
      "learning_rate": 0.00016371584699453553,
      "loss": 0.0005,
      "step": 665
    },
    {
      "epoch": 0.5491803278688525,
      "grad_norm": 0.003649368416517973,
      "learning_rate": 0.0001634426229508197,
      "loss": 0.0003,
      "step": 670
    },
    {
      "epoch": 0.5532786885245902,
      "grad_norm": 0.0032825381495058537,
      "learning_rate": 0.00016316939890710384,
      "loss": 0.0004,
      "step": 675
    },
    {
      "epoch": 0.5573770491803278,
      "grad_norm": 0.0959416851401329,
      "learning_rate": 0.000162896174863388,
      "loss": 0.0035,
      "step": 680
    },
    {
      "epoch": 0.5614754098360656,
      "grad_norm": 0.007344976533204317,
      "learning_rate": 0.00016262295081967214,
      "loss": 0.0003,
      "step": 685
    },
    {
      "epoch": 0.5655737704918032,
      "grad_norm": 0.007972766645252705,
      "learning_rate": 0.0001623497267759563,
      "loss": 0.0022,
      "step": 690
    },
    {
      "epoch": 0.569672131147541,
      "grad_norm": 0.01102761272341013,
      "learning_rate": 0.00016207650273224045,
      "loss": 0.0004,
      "step": 695
    },
    {
      "epoch": 0.5737704918032787,
      "grad_norm": 0.010044329799711704,
      "learning_rate": 0.00016180327868852462,
      "loss": 0.0007,
      "step": 700
    },
    {
      "epoch": 0.5778688524590164,
      "grad_norm": 0.001592421205714345,
      "learning_rate": 0.00016153005464480876,
      "loss": 0.0004,
      "step": 705
    },
    {
      "epoch": 0.5819672131147541,
      "grad_norm": 0.00233217841014266,
      "learning_rate": 0.0001612568306010929,
      "loss": 0.0006,
      "step": 710
    },
    {
      "epoch": 0.5860655737704918,
      "grad_norm": 0.0016034804284572601,
      "learning_rate": 0.00016098360655737707,
      "loss": 0.0003,
      "step": 715
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 0.0024701934307813644,
      "learning_rate": 0.0001607103825136612,
      "loss": 0.0004,
      "step": 720
    },
    {
      "epoch": 0.5942622950819673,
      "grad_norm": 0.0340757817029953,
      "learning_rate": 0.00016043715846994535,
      "loss": 0.0007,
      "step": 725
    },
    {
      "epoch": 0.5983606557377049,
      "grad_norm": 0.00766187347471714,
      "learning_rate": 0.0001601639344262295,
      "loss": 0.0013,
      "step": 730
    },
    {
      "epoch": 0.6024590163934426,
      "grad_norm": 0.0009879113640636206,
      "learning_rate": 0.00015989071038251365,
      "loss": 0.0025,
      "step": 735
    },
    {
      "epoch": 0.6065573770491803,
      "grad_norm": 0.016811899840831757,
      "learning_rate": 0.00015961748633879782,
      "loss": 0.0004,
      "step": 740
    },
    {
      "epoch": 0.610655737704918,
      "grad_norm": 0.0063027977012097836,
      "learning_rate": 0.00015934426229508196,
      "loss": 0.0004,
      "step": 745
    },
    {
      "epoch": 0.6147540983606558,
      "grad_norm": 0.0037851082161068916,
      "learning_rate": 0.00015907103825136613,
      "loss": 0.0008,
      "step": 750
    },
    {
      "epoch": 0.6188524590163934,
      "grad_norm": 0.05159326642751694,
      "learning_rate": 0.00015879781420765027,
      "loss": 0.0005,
      "step": 755
    },
    {
      "epoch": 0.6229508196721312,
      "grad_norm": 0.01401988323777914,
      "learning_rate": 0.00015852459016393443,
      "loss": 0.0004,
      "step": 760
    },
    {
      "epoch": 0.6270491803278688,
      "grad_norm": 0.00021337141515687108,
      "learning_rate": 0.00015825136612021857,
      "loss": 0.0002,
      "step": 765
    },
    {
      "epoch": 0.6311475409836066,
      "grad_norm": 0.00018688617274165154,
      "learning_rate": 0.00015797814207650274,
      "loss": 0.0005,
      "step": 770
    },
    {
      "epoch": 0.6352459016393442,
      "grad_norm": 0.0009013944072648883,
      "learning_rate": 0.00015770491803278688,
      "loss": 0.0002,
      "step": 775
    },
    {
      "epoch": 0.639344262295082,
      "grad_norm": 0.09106282889842987,
      "learning_rate": 0.00015743169398907105,
      "loss": 0.0009,
      "step": 780
    },
    {
      "epoch": 0.6434426229508197,
      "grad_norm": 0.0009247095440514386,
      "learning_rate": 0.0001571584699453552,
      "loss": 0.001,
      "step": 785
    },
    {
      "epoch": 0.6475409836065574,
      "grad_norm": 0.03326667100191116,
      "learning_rate": 0.00015688524590163936,
      "loss": 0.0005,
      "step": 790
    },
    {
      "epoch": 0.6516393442622951,
      "grad_norm": 0.00021822088456247002,
      "learning_rate": 0.0001566120218579235,
      "loss": 0.0003,
      "step": 795
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 0.014621775597333908,
      "learning_rate": 0.00015633879781420766,
      "loss": 0.002,
      "step": 800
    },
    {
      "epoch": 0.6598360655737705,
      "grad_norm": 0.009806141257286072,
      "learning_rate": 0.0001560655737704918,
      "loss": 0.0003,
      "step": 805
    },
    {
      "epoch": 0.6639344262295082,
      "grad_norm": 0.0009548774687573314,
      "learning_rate": 0.00015579234972677597,
      "loss": 0.0007,
      "step": 810
    },
    {
      "epoch": 0.6680327868852459,
      "grad_norm": 0.001779852551408112,
      "learning_rate": 0.0001555191256830601,
      "loss": 0.0004,
      "step": 815
    },
    {
      "epoch": 0.6721311475409836,
      "grad_norm": 0.004381593316793442,
      "learning_rate": 0.00015524590163934428,
      "loss": 0.0009,
      "step": 820
    },
    {
      "epoch": 0.6762295081967213,
      "grad_norm": 0.004107135348021984,
      "learning_rate": 0.00015497267759562842,
      "loss": 0.0006,
      "step": 825
    },
    {
      "epoch": 0.680327868852459,
      "grad_norm": 0.001672861399129033,
      "learning_rate": 0.00015469945355191258,
      "loss": 0.0009,
      "step": 830
    },
    {
      "epoch": 0.6844262295081968,
      "grad_norm": 0.009800495579838753,
      "learning_rate": 0.00015442622950819672,
      "loss": 0.0007,
      "step": 835
    },
    {
      "epoch": 0.6885245901639344,
      "grad_norm": 0.0028825171757489443,
      "learning_rate": 0.0001541530054644809,
      "loss": 0.0002,
      "step": 840
    },
    {
      "epoch": 0.6926229508196722,
      "grad_norm": 0.002611017320305109,
      "learning_rate": 0.00015387978142076506,
      "loss": 0.0003,
      "step": 845
    },
    {
      "epoch": 0.6967213114754098,
      "grad_norm": 0.011444499716162682,
      "learning_rate": 0.0001536065573770492,
      "loss": 0.0014,
      "step": 850
    },
    {
      "epoch": 0.7008196721311475,
      "grad_norm": 0.002013763878494501,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.0004,
      "step": 855
    },
    {
      "epoch": 0.7049180327868853,
      "grad_norm": 0.01398982759565115,
      "learning_rate": 0.0001530601092896175,
      "loss": 0.0005,
      "step": 860
    },
    {
      "epoch": 0.7090163934426229,
      "grad_norm": 0.0059020547196269035,
      "learning_rate": 0.00015278688524590165,
      "loss": 0.0005,
      "step": 865
    },
    {
      "epoch": 0.7131147540983607,
      "grad_norm": 0.0,
      "learning_rate": 0.00015251366120218579,
      "loss": 0.0004,
      "step": 870
    },
    {
      "epoch": 0.7172131147540983,
      "grad_norm": 0.00707361102104187,
      "learning_rate": 0.00015224043715846995,
      "loss": 0.0003,
      "step": 875
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 0.006944413762539625,
      "learning_rate": 0.0001519672131147541,
      "loss": 0.0008,
      "step": 880
    },
    {
      "epoch": 0.7254098360655737,
      "grad_norm": 0.012478955090045929,
      "learning_rate": 0.00015169398907103826,
      "loss": 0.0003,
      "step": 885
    },
    {
      "epoch": 0.7295081967213115,
      "grad_norm": 0.00017574291268829256,
      "learning_rate": 0.0001514207650273224,
      "loss": 0.0006,
      "step": 890
    },
    {
      "epoch": 0.7336065573770492,
      "grad_norm": 0.002650270704180002,
      "learning_rate": 0.00015114754098360657,
      "loss": 0.0002,
      "step": 895
    },
    {
      "epoch": 0.7377049180327869,
      "grad_norm": 0.0011695773573592305,
      "learning_rate": 0.0001508743169398907,
      "loss": 0.0004,
      "step": 900
    },
    {
      "epoch": 0.7418032786885246,
      "grad_norm": 0.0007104747928678989,
      "learning_rate": 0.00015060109289617487,
      "loss": 0.0003,
      "step": 905
    },
    {
      "epoch": 0.7459016393442623,
      "grad_norm": 0.013166557997465134,
      "learning_rate": 0.000150327868852459,
      "loss": 0.0005,
      "step": 910
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.002409046981483698,
      "learning_rate": 0.00015005464480874318,
      "loss": 0.0002,
      "step": 915
    },
    {
      "epoch": 0.7540983606557377,
      "grad_norm": 0.0009846242610365152,
      "learning_rate": 0.00014978142076502732,
      "loss": 0.0016,
      "step": 920
    },
    {
      "epoch": 0.7581967213114754,
      "grad_norm": 0.006381461396813393,
      "learning_rate": 0.0001495081967213115,
      "loss": 0.0003,
      "step": 925
    },
    {
      "epoch": 0.7622950819672131,
      "grad_norm": 0.014935264363884926,
      "learning_rate": 0.00014923497267759563,
      "loss": 0.0005,
      "step": 930
    },
    {
      "epoch": 0.7663934426229508,
      "grad_norm": 2.8892562113469467e-05,
      "learning_rate": 0.0001489617486338798,
      "loss": 0.0002,
      "step": 935
    },
    {
      "epoch": 0.7704918032786885,
      "grad_norm": 5.2531999244820327e-05,
      "learning_rate": 0.00014868852459016393,
      "loss": 0.0002,
      "step": 940
    },
    {
      "epoch": 0.7745901639344263,
      "grad_norm": 0.0003357222012709826,
      "learning_rate": 0.0001484153005464481,
      "loss": 0.0006,
      "step": 945
    },
    {
      "epoch": 0.7786885245901639,
      "grad_norm": 0.0025565708056092262,
      "learning_rate": 0.00014814207650273224,
      "loss": 0.0003,
      "step": 950
    },
    {
      "epoch": 0.7827868852459017,
      "grad_norm": 6.038168794475496e-05,
      "learning_rate": 0.0001478688524590164,
      "loss": 0.0001,
      "step": 955
    },
    {
      "epoch": 0.7868852459016393,
      "grad_norm": 0.0017571707721799612,
      "learning_rate": 0.00014759562841530055,
      "loss": 0.0003,
      "step": 960
    },
    {
      "epoch": 0.7909836065573771,
      "grad_norm": 0.013048162683844566,
      "learning_rate": 0.00014732240437158472,
      "loss": 0.0039,
      "step": 965
    },
    {
      "epoch": 0.7950819672131147,
      "grad_norm": 6.025774382578675e-06,
      "learning_rate": 0.00014704918032786886,
      "loss": 0.0003,
      "step": 970
    },
    {
      "epoch": 0.7991803278688525,
      "grad_norm": 0.0026503284461796284,
      "learning_rate": 0.00014677595628415302,
      "loss": 0.0003,
      "step": 975
    },
    {
      "epoch": 0.8032786885245902,
      "grad_norm": 0.0007452239515259862,
      "learning_rate": 0.00014650273224043716,
      "loss": 0.0003,
      "step": 980
    },
    {
      "epoch": 0.8073770491803278,
      "grad_norm": 0.00042176988790743053,
      "learning_rate": 0.00014622950819672133,
      "loss": 0.0003,
      "step": 985
    },
    {
      "epoch": 0.8114754098360656,
      "grad_norm": 0.006051498930901289,
      "learning_rate": 0.00014595628415300547,
      "loss": 0.0004,
      "step": 990
    },
    {
      "epoch": 0.8155737704918032,
      "grad_norm": 0.001158890314400196,
      "learning_rate": 0.00014568306010928964,
      "loss": 0.0003,
      "step": 995
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 0.009514193050563335,
      "learning_rate": 0.00014540983606557378,
      "loss": 0.0004,
      "step": 1000
    },
    {
      "epoch": 0.8237704918032787,
      "grad_norm": 0.006030993536114693,
      "learning_rate": 0.00014513661202185794,
      "loss": 0.0003,
      "step": 1005
    },
    {
      "epoch": 0.8278688524590164,
      "grad_norm": 0.0015964112244546413,
      "learning_rate": 0.00014486338797814208,
      "loss": 0.0003,
      "step": 1010
    },
    {
      "epoch": 0.8319672131147541,
      "grad_norm": 0.00158079678658396,
      "learning_rate": 0.00014459016393442622,
      "loss": 0.0003,
      "step": 1015
    },
    {
      "epoch": 0.8360655737704918,
      "grad_norm": 0.0019842267502099276,
      "learning_rate": 0.0001443169398907104,
      "loss": 0.0004,
      "step": 1020
    },
    {
      "epoch": 0.8401639344262295,
      "grad_norm": 0.00946321152150631,
      "learning_rate": 0.00014404371584699453,
      "loss": 0.0003,
      "step": 1025
    },
    {
      "epoch": 0.8442622950819673,
      "grad_norm": 0.0010287526529282331,
      "learning_rate": 0.0001437704918032787,
      "loss": 0.0005,
      "step": 1030
    },
    {
      "epoch": 0.8483606557377049,
      "grad_norm": 0.00543291587382555,
      "learning_rate": 0.00014349726775956284,
      "loss": 0.0002,
      "step": 1035
    },
    {
      "epoch": 0.8524590163934426,
      "grad_norm": 0.00014987921167630702,
      "learning_rate": 0.000143224043715847,
      "loss": 0.0006,
      "step": 1040
    },
    {
      "epoch": 0.8565573770491803,
      "grad_norm": 0.001962999813258648,
      "learning_rate": 0.00014295081967213115,
      "loss": 0.0003,
      "step": 1045
    },
    {
      "epoch": 0.860655737704918,
      "grad_norm": 0.008594807237386703,
      "learning_rate": 0.0001426775956284153,
      "loss": 0.0009,
      "step": 1050
    },
    {
      "epoch": 0.8647540983606558,
      "grad_norm": 0.0005910912877880037,
      "learning_rate": 0.00014240437158469945,
      "loss": 0.0003,
      "step": 1055
    },
    {
      "epoch": 0.8688524590163934,
      "grad_norm": 0.006622785702347755,
      "learning_rate": 0.00014213114754098362,
      "loss": 0.0006,
      "step": 1060
    },
    {
      "epoch": 0.8729508196721312,
      "grad_norm": 0.000164223529282026,
      "learning_rate": 0.00014185792349726776,
      "loss": 0.0008,
      "step": 1065
    },
    {
      "epoch": 0.8770491803278688,
      "grad_norm": 0.02034379169344902,
      "learning_rate": 0.00014158469945355193,
      "loss": 0.0005,
      "step": 1070
    },
    {
      "epoch": 0.8811475409836066,
      "grad_norm": 0.12764950096607208,
      "learning_rate": 0.00014131147540983607,
      "loss": 0.0009,
      "step": 1075
    },
    {
      "epoch": 0.8852459016393442,
      "grad_norm": 0.003921673633158207,
      "learning_rate": 0.00014103825136612023,
      "loss": 0.0004,
      "step": 1080
    },
    {
      "epoch": 0.889344262295082,
      "grad_norm": 0.0036915477830916643,
      "learning_rate": 0.00014076502732240437,
      "loss": 0.0004,
      "step": 1085
    },
    {
      "epoch": 0.8934426229508197,
      "grad_norm": 0.009272467344999313,
      "learning_rate": 0.00014049180327868854,
      "loss": 0.0002,
      "step": 1090
    },
    {
      "epoch": 0.8975409836065574,
      "grad_norm": 0.0013436581939458847,
      "learning_rate": 0.00014021857923497268,
      "loss": 0.0005,
      "step": 1095
    },
    {
      "epoch": 0.9016393442622951,
      "grad_norm": 0.00018279057985637337,
      "learning_rate": 0.00013994535519125685,
      "loss": 0.0001,
      "step": 1100
    },
    {
      "epoch": 0.9057377049180327,
      "grad_norm": 0.0003492105461191386,
      "learning_rate": 0.000139672131147541,
      "loss": 0.0002,
      "step": 1105
    },
    {
      "epoch": 0.9098360655737705,
      "grad_norm": 0.01673228107392788,
      "learning_rate": 0.00013939890710382516,
      "loss": 0.0009,
      "step": 1110
    },
    {
      "epoch": 0.9139344262295082,
      "grad_norm": 0.0019281478598713875,
      "learning_rate": 0.0001391256830601093,
      "loss": 0.0004,
      "step": 1115
    },
    {
      "epoch": 0.9180327868852459,
      "grad_norm": 0.006077837198972702,
      "learning_rate": 0.00013885245901639346,
      "loss": 0.0002,
      "step": 1120
    },
    {
      "epoch": 0.9221311475409836,
      "grad_norm": 0.0013992706080898643,
      "learning_rate": 0.0001385792349726776,
      "loss": 0.0003,
      "step": 1125
    },
    {
      "epoch": 0.9262295081967213,
      "grad_norm": 0.0016170593444257975,
      "learning_rate": 0.00013830601092896177,
      "loss": 0.0005,
      "step": 1130
    },
    {
      "epoch": 0.930327868852459,
      "grad_norm": 0.0010380421299487352,
      "learning_rate": 0.0001380327868852459,
      "loss": 0.0002,
      "step": 1135
    },
    {
      "epoch": 0.9344262295081968,
      "grad_norm": 0.006002561654895544,
      "learning_rate": 0.00013775956284153008,
      "loss": 0.0021,
      "step": 1140
    },
    {
      "epoch": 0.9385245901639344,
      "grad_norm": 0.0030343758407980204,
      "learning_rate": 0.00013748633879781422,
      "loss": 0.0004,
      "step": 1145
    },
    {
      "epoch": 0.9426229508196722,
      "grad_norm": 0.06433916836977005,
      "learning_rate": 0.00013721311475409838,
      "loss": 0.0007,
      "step": 1150
    },
    {
      "epoch": 0.9467213114754098,
      "grad_norm": 0.001449406030587852,
      "learning_rate": 0.00013693989071038252,
      "loss": 0.0002,
      "step": 1155
    },
    {
      "epoch": 0.9508196721311475,
      "grad_norm": 0.00058654451277107,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.0002,
      "step": 1160
    },
    {
      "epoch": 0.9549180327868853,
      "grad_norm": 0.0018500920850783587,
      "learning_rate": 0.00013639344262295083,
      "loss": 0.0002,
      "step": 1165
    },
    {
      "epoch": 0.9590163934426229,
      "grad_norm": 0.0007623726851306856,
      "learning_rate": 0.00013612021857923497,
      "loss": 0.0007,
      "step": 1170
    },
    {
      "epoch": 0.9631147540983607,
      "grad_norm": 0.0048635173588991165,
      "learning_rate": 0.0001358469945355191,
      "loss": 0.0004,
      "step": 1175
    },
    {
      "epoch": 0.9672131147540983,
      "grad_norm": 0.0033725278917700052,
      "learning_rate": 0.00013557377049180328,
      "loss": 0.0004,
      "step": 1180
    },
    {
      "epoch": 0.9713114754098361,
      "grad_norm": 0.0025824429467320442,
      "learning_rate": 0.00013530054644808742,
      "loss": 0.0006,
      "step": 1185
    },
    {
      "epoch": 0.9754098360655737,
      "grad_norm": 0.01809200458228588,
      "learning_rate": 0.00013502732240437159,
      "loss": 0.0006,
      "step": 1190
    },
    {
      "epoch": 0.9795081967213115,
      "grad_norm": 0.0017302167834714055,
      "learning_rate": 0.00013475409836065573,
      "loss": 0.0005,
      "step": 1195
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 0.00015758209337946028,
      "learning_rate": 0.0001344808743169399,
      "loss": 0.0004,
      "step": 1200
    },
    {
      "epoch": 0.9877049180327869,
      "grad_norm": 0.001532477792352438,
      "learning_rate": 0.00013420765027322403,
      "loss": 0.0001,
      "step": 1205
    },
    {
      "epoch": 0.9918032786885246,
      "grad_norm": 0.0008180724689736962,
      "learning_rate": 0.0001339344262295082,
      "loss": 0.0003,
      "step": 1210
    },
    {
      "epoch": 0.9959016393442623,
      "grad_norm": 0.000277614570222795,
      "learning_rate": 0.00013366120218579237,
      "loss": 0.0002,
      "step": 1215
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.002776428824290633,
      "learning_rate": 0.0001333879781420765,
      "loss": 0.0002,
      "step": 1220
    },
    {
      "epoch": 1.0040983606557377,
      "grad_norm": 0.02655249461531639,
      "learning_rate": 0.00013311475409836067,
      "loss": 0.0004,
      "step": 1225
    },
    {
      "epoch": 1.0081967213114753,
      "grad_norm": 0.0006007161573506892,
      "learning_rate": 0.0001328415300546448,
      "loss": 0.0001,
      "step": 1230
    },
    {
      "epoch": 1.0122950819672132,
      "grad_norm": 0.029652189463377,
      "learning_rate": 0.00013256830601092898,
      "loss": 0.0005,
      "step": 1235
    },
    {
      "epoch": 1.0163934426229508,
      "grad_norm": 0.0017862051026895642,
      "learning_rate": 0.00013229508196721312,
      "loss": 0.0001,
      "step": 1240
    },
    {
      "epoch": 1.0204918032786885,
      "grad_norm": 2.598354512883816e-05,
      "learning_rate": 0.0001320218579234973,
      "loss": 0.0009,
      "step": 1245
    },
    {
      "epoch": 1.0245901639344261,
      "grad_norm": 0.00035268397186882794,
      "learning_rate": 0.00013174863387978143,
      "loss": 0.0007,
      "step": 1250
    },
    {
      "epoch": 1.028688524590164,
      "grad_norm": 0.003662915900349617,
      "learning_rate": 0.0001314754098360656,
      "loss": 0.0002,
      "step": 1255
    },
    {
      "epoch": 1.0327868852459017,
      "grad_norm": 0.002379933139309287,
      "learning_rate": 0.00013120218579234973,
      "loss": 0.0004,
      "step": 1260
    },
    {
      "epoch": 1.0368852459016393,
      "grad_norm": 0.006878622341901064,
      "learning_rate": 0.0001309289617486339,
      "loss": 0.0002,
      "step": 1265
    },
    {
      "epoch": 1.040983606557377,
      "grad_norm": 0.0008950517512857914,
      "learning_rate": 0.00013065573770491804,
      "loss": 0.0003,
      "step": 1270
    },
    {
      "epoch": 1.0450819672131149,
      "grad_norm": 0.0007496739272028208,
      "learning_rate": 0.0001303825136612022,
      "loss": 0.0011,
      "step": 1275
    },
    {
      "epoch": 1.0491803278688525,
      "grad_norm": 0.0030558574944734573,
      "learning_rate": 0.00013010928961748635,
      "loss": 0.0002,
      "step": 1280
    },
    {
      "epoch": 1.0532786885245902,
      "grad_norm": 0.0005652621039189398,
      "learning_rate": 0.00012983606557377052,
      "loss": 0.0001,
      "step": 1285
    },
    {
      "epoch": 1.0573770491803278,
      "grad_norm": 0.00020617105474229902,
      "learning_rate": 0.00012956284153005466,
      "loss": 0.0002,
      "step": 1290
    },
    {
      "epoch": 1.0614754098360655,
      "grad_norm": 0.0011703720083460212,
      "learning_rate": 0.00012928961748633882,
      "loss": 0.0009,
      "step": 1295
    },
    {
      "epoch": 1.0655737704918034,
      "grad_norm": 0.00044316850835457444,
      "learning_rate": 0.00012901639344262296,
      "loss": 0.0001,
      "step": 1300
    },
    {
      "epoch": 1.069672131147541,
      "grad_norm": 0.00299637159332633,
      "learning_rate": 0.0001287431693989071,
      "loss": 0.0003,
      "step": 1305
    },
    {
      "epoch": 1.0737704918032787,
      "grad_norm": 0.0004825017531402409,
      "learning_rate": 0.00012846994535519127,
      "loss": 0.0002,
      "step": 1310
    },
    {
      "epoch": 1.0778688524590163,
      "grad_norm": 0.009396213106811047,
      "learning_rate": 0.0001281967213114754,
      "loss": 0.0002,
      "step": 1315
    },
    {
      "epoch": 1.0819672131147542,
      "grad_norm": 0.0004743629542645067,
      "learning_rate": 0.00012792349726775955,
      "loss": 0.0005,
      "step": 1320
    },
    {
      "epoch": 1.0860655737704918,
      "grad_norm": 0.007257436402142048,
      "learning_rate": 0.00012765027322404372,
      "loss": 0.0003,
      "step": 1325
    },
    {
      "epoch": 1.0901639344262295,
      "grad_norm": 0.009631735272705555,
      "learning_rate": 0.00012737704918032786,
      "loss": 0.0004,
      "step": 1330
    },
    {
      "epoch": 1.0942622950819672,
      "grad_norm": 0.006310819182544947,
      "learning_rate": 0.00012710382513661202,
      "loss": 0.0003,
      "step": 1335
    },
    {
      "epoch": 1.098360655737705,
      "grad_norm": 0.00030461454298347235,
      "learning_rate": 0.00012683060109289616,
      "loss": 0.0002,
      "step": 1340
    },
    {
      "epoch": 1.1024590163934427,
      "grad_norm": 0.003052534768357873,
      "learning_rate": 0.00012655737704918033,
      "loss": 0.0002,
      "step": 1345
    },
    {
      "epoch": 1.1065573770491803,
      "grad_norm": 0.011136494576931,
      "learning_rate": 0.00012628415300546447,
      "loss": 0.0008,
      "step": 1350
    },
    {
      "epoch": 1.110655737704918,
      "grad_norm": 0.0015124179190024734,
      "learning_rate": 0.00012601092896174864,
      "loss": 0.0002,
      "step": 1355
    },
    {
      "epoch": 1.1147540983606556,
      "grad_norm": 0.00042045448208227754,
      "learning_rate": 0.00012573770491803278,
      "loss": 0.0001,
      "step": 1360
    },
    {
      "epoch": 1.1188524590163935,
      "grad_norm": 0.0016602784162387252,
      "learning_rate": 0.00012546448087431695,
      "loss": 0.0002,
      "step": 1365
    },
    {
      "epoch": 1.1229508196721312,
      "grad_norm": 7.617013034177944e-05,
      "learning_rate": 0.00012519125683060109,
      "loss": 0.0003,
      "step": 1370
    },
    {
      "epoch": 1.1270491803278688,
      "grad_norm": 1.3596145436167717e-05,
      "learning_rate": 0.00012491803278688525,
      "loss": 0.0001,
      "step": 1375
    },
    {
      "epoch": 1.1311475409836065,
      "grad_norm": 0.00039830897003412247,
      "learning_rate": 0.0001246448087431694,
      "loss": 0.0001,
      "step": 1380
    },
    {
      "epoch": 1.1352459016393444,
      "grad_norm": 0.004205585923045874,
      "learning_rate": 0.00012437158469945356,
      "loss": 0.0002,
      "step": 1385
    },
    {
      "epoch": 1.139344262295082,
      "grad_norm": 0.0023438837379217148,
      "learning_rate": 0.0001240983606557377,
      "loss": 0.0003,
      "step": 1390
    },
    {
      "epoch": 1.1434426229508197,
      "grad_norm": 0.0018822313286364079,
      "learning_rate": 0.00012382513661202187,
      "loss": 0.0001,
      "step": 1395
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 0.0025507837999612093,
      "learning_rate": 0.00012355191256830603,
      "loss": 0.0003,
      "step": 1400
    },
    {
      "epoch": 1.151639344262295,
      "grad_norm": 0.001969727221876383,
      "learning_rate": 0.00012327868852459017,
      "loss": 0.0002,
      "step": 1405
    },
    {
      "epoch": 1.1557377049180328,
      "grad_norm": 0.004126941319555044,
      "learning_rate": 0.00012300546448087434,
      "loss": 0.0002,
      "step": 1410
    },
    {
      "epoch": 1.1598360655737705,
      "grad_norm": 6.033002136973664e-05,
      "learning_rate": 0.00012273224043715848,
      "loss": 0.0003,
      "step": 1415
    },
    {
      "epoch": 1.1639344262295082,
      "grad_norm": 0.0012537690345197916,
      "learning_rate": 0.00012245901639344265,
      "loss": 0.0002,
      "step": 1420
    },
    {
      "epoch": 1.1680327868852458,
      "grad_norm": 0.0018491378286853433,
      "learning_rate": 0.0001221857923497268,
      "loss": 0.0002,
      "step": 1425
    },
    {
      "epoch": 1.1721311475409837,
      "grad_norm": 0.02502857707440853,
      "learning_rate": 0.00012191256830601094,
      "loss": 0.0003,
      "step": 1430
    },
    {
      "epoch": 1.1762295081967213,
      "grad_norm": 0.011267426423728466,
      "learning_rate": 0.00012163934426229508,
      "loss": 0.0003,
      "step": 1435
    },
    {
      "epoch": 1.180327868852459,
      "grad_norm": 0.0023777021560817957,
      "learning_rate": 0.00012136612021857925,
      "loss": 0.0002,
      "step": 1440
    },
    {
      "epoch": 1.1844262295081966,
      "grad_norm": 0.0015517434803768992,
      "learning_rate": 0.00012109289617486339,
      "loss": 0.0003,
      "step": 1445
    },
    {
      "epoch": 1.1885245901639343,
      "grad_norm": 3.5332664992893115e-05,
      "learning_rate": 0.00012081967213114756,
      "loss": 0.0003,
      "step": 1450
    },
    {
      "epoch": 1.1926229508196722,
      "grad_norm": 0.0004914128221571445,
      "learning_rate": 0.0001205464480874317,
      "loss": 0.0003,
      "step": 1455
    },
    {
      "epoch": 1.1967213114754098,
      "grad_norm": 0.004731389228254557,
      "learning_rate": 0.00012027322404371586,
      "loss": 0.0004,
      "step": 1460
    },
    {
      "epoch": 1.2008196721311475,
      "grad_norm": 0.0003742514527402818,
      "learning_rate": 0.00012,
      "loss": 0.0002,
      "step": 1465
    },
    {
      "epoch": 1.2049180327868854,
      "grad_norm": 0.00461757043376565,
      "learning_rate": 0.00011972677595628417,
      "loss": 0.0002,
      "step": 1470
    },
    {
      "epoch": 1.209016393442623,
      "grad_norm": 2.0190256691421382e-05,
      "learning_rate": 0.00011945355191256831,
      "loss": 0.0003,
      "step": 1475
    },
    {
      "epoch": 1.2131147540983607,
      "grad_norm": 0.007319913245737553,
      "learning_rate": 0.00011918032786885248,
      "loss": 0.0239,
      "step": 1480
    },
    {
      "epoch": 1.2172131147540983,
      "grad_norm": 0.0005722435307689011,
      "learning_rate": 0.00011890710382513662,
      "loss": 0.0001,
      "step": 1485
    },
    {
      "epoch": 1.221311475409836,
      "grad_norm": 0.0010803983313962817,
      "learning_rate": 0.00011863387978142077,
      "loss": 0.0001,
      "step": 1490
    },
    {
      "epoch": 1.2254098360655739,
      "grad_norm": 0.00052156817400828,
      "learning_rate": 0.00011836065573770491,
      "loss": 0.0005,
      "step": 1495
    },
    {
      "epoch": 1.2295081967213115,
      "grad_norm": 0.00038197298999875784,
      "learning_rate": 0.00011808743169398908,
      "loss": 0.0003,
      "step": 1500
    }
  ],
  "logging_steps": 5,
  "max_steps": 3660,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 409147050645504.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
