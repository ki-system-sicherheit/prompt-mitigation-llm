{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4098360655737705,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004098360655737705,
      "grad_norm": 0.9527931213378906,
      "learning_rate": 0.00019978142076502732,
      "loss": 0.8217,
      "step": 5
    },
    {
      "epoch": 0.00819672131147541,
      "grad_norm": 0.5045624375343323,
      "learning_rate": 0.00019950819672131148,
      "loss": 0.6988,
      "step": 10
    },
    {
      "epoch": 0.012295081967213115,
      "grad_norm": 0.6213607788085938,
      "learning_rate": 0.00019923497267759562,
      "loss": 0.5741,
      "step": 15
    },
    {
      "epoch": 0.01639344262295082,
      "grad_norm": 1.107638955116272,
      "learning_rate": 0.0001989617486338798,
      "loss": 0.4477,
      "step": 20
    },
    {
      "epoch": 0.020491803278688523,
      "grad_norm": 0.4475697875022888,
      "learning_rate": 0.00019868852459016393,
      "loss": 0.2998,
      "step": 25
    },
    {
      "epoch": 0.02459016393442623,
      "grad_norm": 0.43611767888069153,
      "learning_rate": 0.0001984153005464481,
      "loss": 0.1331,
      "step": 30
    },
    {
      "epoch": 0.028688524590163935,
      "grad_norm": 0.9668943285942078,
      "learning_rate": 0.00019814207650273224,
      "loss": 0.1277,
      "step": 35
    },
    {
      "epoch": 0.03278688524590164,
      "grad_norm": 0.7083913683891296,
      "learning_rate": 0.0001978688524590164,
      "loss": 0.1006,
      "step": 40
    },
    {
      "epoch": 0.036885245901639344,
      "grad_norm": 0.8571495413780212,
      "learning_rate": 0.00019759562841530054,
      "loss": 0.0931,
      "step": 45
    },
    {
      "epoch": 0.040983606557377046,
      "grad_norm": 1.0128223896026611,
      "learning_rate": 0.0001973224043715847,
      "loss": 0.0423,
      "step": 50
    },
    {
      "epoch": 0.045081967213114756,
      "grad_norm": 0.185210719704628,
      "learning_rate": 0.00019704918032786885,
      "loss": 0.0145,
      "step": 55
    },
    {
      "epoch": 0.04918032786885246,
      "grad_norm": 0.2549950182437897,
      "learning_rate": 0.00019677595628415302,
      "loss": 0.0202,
      "step": 60
    },
    {
      "epoch": 0.05327868852459016,
      "grad_norm": 0.011635084636509418,
      "learning_rate": 0.00019650273224043716,
      "loss": 0.0239,
      "step": 65
    },
    {
      "epoch": 0.05737704918032787,
      "grad_norm": 0.07656217366456985,
      "learning_rate": 0.00019622950819672133,
      "loss": 0.0074,
      "step": 70
    },
    {
      "epoch": 0.06147540983606557,
      "grad_norm": 0.060497161000967026,
      "learning_rate": 0.00019595628415300547,
      "loss": 0.0276,
      "step": 75
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 0.07262999564409256,
      "learning_rate": 0.00019568306010928963,
      "loss": 0.0085,
      "step": 80
    },
    {
      "epoch": 0.06967213114754098,
      "grad_norm": 0.030240889638662338,
      "learning_rate": 0.00019540983606557377,
      "loss": 0.0083,
      "step": 85
    },
    {
      "epoch": 0.07377049180327869,
      "grad_norm": 0.04026243835687637,
      "learning_rate": 0.00019513661202185794,
      "loss": 0.0052,
      "step": 90
    },
    {
      "epoch": 0.0778688524590164,
      "grad_norm": 0.19012518227100372,
      "learning_rate": 0.00019486338797814208,
      "loss": 0.0061,
      "step": 95
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 0.16719123721122742,
      "learning_rate": 0.00019459016393442625,
      "loss": 0.0101,
      "step": 100
    },
    {
      "epoch": 0.0860655737704918,
      "grad_norm": 0.017311835661530495,
      "learning_rate": 0.0001943169398907104,
      "loss": 0.0034,
      "step": 105
    },
    {
      "epoch": 0.09016393442622951,
      "grad_norm": 0.021288778632879257,
      "learning_rate": 0.00019404371584699455,
      "loss": 0.0078,
      "step": 110
    },
    {
      "epoch": 0.0942622950819672,
      "grad_norm": 0.0617251843214035,
      "learning_rate": 0.00019377049180327872,
      "loss": 0.0026,
      "step": 115
    },
    {
      "epoch": 0.09836065573770492,
      "grad_norm": 0.8364461660385132,
      "learning_rate": 0.00019349726775956286,
      "loss": 0.0068,
      "step": 120
    },
    {
      "epoch": 0.10245901639344263,
      "grad_norm": 0.0058639030903577805,
      "learning_rate": 0.000193224043715847,
      "loss": 0.0041,
      "step": 125
    },
    {
      "epoch": 0.10655737704918032,
      "grad_norm": 0.04045857861638069,
      "learning_rate": 0.00019295081967213117,
      "loss": 0.0072,
      "step": 130
    },
    {
      "epoch": 0.11065573770491803,
      "grad_norm": 1.3383764028549194,
      "learning_rate": 0.0001926775956284153,
      "loss": 0.0182,
      "step": 135
    },
    {
      "epoch": 0.11475409836065574,
      "grad_norm": 0.1323436200618744,
      "learning_rate": 0.00019240437158469945,
      "loss": 0.0021,
      "step": 140
    },
    {
      "epoch": 0.11885245901639344,
      "grad_norm": 0.025622591376304626,
      "learning_rate": 0.00019213114754098362,
      "loss": 0.0046,
      "step": 145
    },
    {
      "epoch": 0.12295081967213115,
      "grad_norm": 0.010446264408528805,
      "learning_rate": 0.00019185792349726776,
      "loss": 0.0027,
      "step": 150
    },
    {
      "epoch": 0.12704918032786885,
      "grad_norm": 0.03192673996090889,
      "learning_rate": 0.00019158469945355192,
      "loss": 0.0023,
      "step": 155
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 0.009083353914320469,
      "learning_rate": 0.00019131147540983606,
      "loss": 0.0012,
      "step": 160
    },
    {
      "epoch": 0.13524590163934427,
      "grad_norm": 0.01632789522409439,
      "learning_rate": 0.00019103825136612023,
      "loss": 0.002,
      "step": 165
    },
    {
      "epoch": 0.13934426229508196,
      "grad_norm": 0.03792854771018028,
      "learning_rate": 0.00019076502732240437,
      "loss": 0.0018,
      "step": 170
    },
    {
      "epoch": 0.14344262295081966,
      "grad_norm": 0.049662161618471146,
      "learning_rate": 0.00019049180327868854,
      "loss": 0.0056,
      "step": 175
    },
    {
      "epoch": 0.14754098360655737,
      "grad_norm": 0.004366317763924599,
      "learning_rate": 0.00019021857923497268,
      "loss": 0.0045,
      "step": 180
    },
    {
      "epoch": 0.15163934426229508,
      "grad_norm": 0.0021171499975025654,
      "learning_rate": 0.00018994535519125684,
      "loss": 0.0047,
      "step": 185
    },
    {
      "epoch": 0.1557377049180328,
      "grad_norm": 0.04822837933897972,
      "learning_rate": 0.00018967213114754098,
      "loss": 0.0016,
      "step": 190
    },
    {
      "epoch": 0.1598360655737705,
      "grad_norm": 0.01301040593534708,
      "learning_rate": 0.00018939890710382515,
      "loss": 0.001,
      "step": 195
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 0.09617028385400772,
      "learning_rate": 0.0001891256830601093,
      "loss": 0.0022,
      "step": 200
    },
    {
      "epoch": 0.1680327868852459,
      "grad_norm": 0.0021836638916283846,
      "learning_rate": 0.00018885245901639346,
      "loss": 0.0015,
      "step": 205
    },
    {
      "epoch": 0.1721311475409836,
      "grad_norm": 0.017412008717656136,
      "learning_rate": 0.0001885792349726776,
      "loss": 0.0021,
      "step": 210
    },
    {
      "epoch": 0.1762295081967213,
      "grad_norm": 0.008060822263360023,
      "learning_rate": 0.00018830601092896177,
      "loss": 0.0007,
      "step": 215
    },
    {
      "epoch": 0.18032786885245902,
      "grad_norm": 0.02488826774060726,
      "learning_rate": 0.0001880327868852459,
      "loss": 0.0024,
      "step": 220
    },
    {
      "epoch": 0.18442622950819673,
      "grad_norm": 0.005369509104639292,
      "learning_rate": 0.00018775956284153007,
      "loss": 0.0017,
      "step": 225
    },
    {
      "epoch": 0.1885245901639344,
      "grad_norm": 0.02311077155172825,
      "learning_rate": 0.0001874863387978142,
      "loss": 0.001,
      "step": 230
    },
    {
      "epoch": 0.19262295081967212,
      "grad_norm": 0.018028857186436653,
      "learning_rate": 0.00018721311475409838,
      "loss": 0.0024,
      "step": 235
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 0.004582926630973816,
      "learning_rate": 0.00018693989071038252,
      "loss": 0.0015,
      "step": 240
    },
    {
      "epoch": 0.20081967213114754,
      "grad_norm": 0.01514498796314001,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.0022,
      "step": 245
    },
    {
      "epoch": 0.20491803278688525,
      "grad_norm": 0.009839089587330818,
      "learning_rate": 0.00018639344262295083,
      "loss": 0.0053,
      "step": 250
    },
    {
      "epoch": 0.20901639344262296,
      "grad_norm": 1.1162108182907104,
      "learning_rate": 0.000186120218579235,
      "loss": 0.0089,
      "step": 255
    },
    {
      "epoch": 0.21311475409836064,
      "grad_norm": 0.009776981547474861,
      "learning_rate": 0.00018584699453551913,
      "loss": 0.001,
      "step": 260
    },
    {
      "epoch": 0.21721311475409835,
      "grad_norm": 0.0063355970196425915,
      "learning_rate": 0.0001855737704918033,
      "loss": 0.0017,
      "step": 265
    },
    {
      "epoch": 0.22131147540983606,
      "grad_norm": 0.005864137317985296,
      "learning_rate": 0.00018530054644808744,
      "loss": 0.0018,
      "step": 270
    },
    {
      "epoch": 0.22540983606557377,
      "grad_norm": 0.07048726826906204,
      "learning_rate": 0.0001850273224043716,
      "loss": 0.0019,
      "step": 275
    },
    {
      "epoch": 0.22950819672131148,
      "grad_norm": 0.02692907676100731,
      "learning_rate": 0.00018475409836065575,
      "loss": 0.0012,
      "step": 280
    },
    {
      "epoch": 0.2336065573770492,
      "grad_norm": 0.01985354721546173,
      "learning_rate": 0.0001844808743169399,
      "loss": 0.0015,
      "step": 285
    },
    {
      "epoch": 0.23770491803278687,
      "grad_norm": 0.002445412566885352,
      "learning_rate": 0.00018420765027322405,
      "loss": 0.0012,
      "step": 290
    },
    {
      "epoch": 0.24180327868852458,
      "grad_norm": 0.0151666896417737,
      "learning_rate": 0.0001839344262295082,
      "loss": 0.0008,
      "step": 295
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 0.008400567807257175,
      "learning_rate": 0.00018366120218579236,
      "loss": 0.0022,
      "step": 300
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01348680816590786,
      "learning_rate": 0.0001833879781420765,
      "loss": 0.0038,
      "step": 305
    },
    {
      "epoch": 0.2540983606557377,
      "grad_norm": 0.008146454580128193,
      "learning_rate": 0.00018311475409836067,
      "loss": 0.0006,
      "step": 310
    },
    {
      "epoch": 0.2581967213114754,
      "grad_norm": 0.07796267420053482,
      "learning_rate": 0.0001828415300546448,
      "loss": 0.0021,
      "step": 315
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 0.1860826313495636,
      "learning_rate": 0.00018256830601092898,
      "loss": 0.0072,
      "step": 320
    },
    {
      "epoch": 0.26639344262295084,
      "grad_norm": 0.0037070682737976313,
      "learning_rate": 0.00018229508196721312,
      "loss": 0.0012,
      "step": 325
    },
    {
      "epoch": 0.27049180327868855,
      "grad_norm": 0.001117024919949472,
      "learning_rate": 0.00018202185792349728,
      "loss": 0.0004,
      "step": 330
    },
    {
      "epoch": 0.27459016393442626,
      "grad_norm": 0.1939241886138916,
      "learning_rate": 0.00018174863387978142,
      "loss": 0.0063,
      "step": 335
    },
    {
      "epoch": 0.2786885245901639,
      "grad_norm": 0.020207803696393967,
      "learning_rate": 0.0001814754098360656,
      "loss": 0.001,
      "step": 340
    },
    {
      "epoch": 0.2827868852459016,
      "grad_norm": 0.010100802406668663,
      "learning_rate": 0.00018120218579234973,
      "loss": 0.0004,
      "step": 345
    },
    {
      "epoch": 0.28688524590163933,
      "grad_norm": 0.012284435331821442,
      "learning_rate": 0.0001809289617486339,
      "loss": 0.0047,
      "step": 350
    },
    {
      "epoch": 0.29098360655737704,
      "grad_norm": 0.026365771889686584,
      "learning_rate": 0.00018065573770491804,
      "loss": 0.0011,
      "step": 355
    },
    {
      "epoch": 0.29508196721311475,
      "grad_norm": 0.0016114693135023117,
      "learning_rate": 0.0001803825136612022,
      "loss": 0.001,
      "step": 360
    },
    {
      "epoch": 0.29918032786885246,
      "grad_norm": 0.0032059005461633205,
      "learning_rate": 0.00018010928961748634,
      "loss": 0.0023,
      "step": 365
    },
    {
      "epoch": 0.30327868852459017,
      "grad_norm": 0.055000223219394684,
      "learning_rate": 0.0001798360655737705,
      "loss": 0.0019,
      "step": 370
    },
    {
      "epoch": 0.3073770491803279,
      "grad_norm": 0.0024290892761200666,
      "learning_rate": 0.00017956284153005465,
      "loss": 0.0009,
      "step": 375
    },
    {
      "epoch": 0.3114754098360656,
      "grad_norm": 0.016742771491408348,
      "learning_rate": 0.00017928961748633882,
      "loss": 0.0008,
      "step": 380
    },
    {
      "epoch": 0.3155737704918033,
      "grad_norm": 0.002140327822417021,
      "learning_rate": 0.00017901639344262296,
      "loss": 0.0007,
      "step": 385
    },
    {
      "epoch": 0.319672131147541,
      "grad_norm": 0.020881978794932365,
      "learning_rate": 0.00017874316939890713,
      "loss": 0.0037,
      "step": 390
    },
    {
      "epoch": 0.3237704918032787,
      "grad_norm": 0.007238979451358318,
      "learning_rate": 0.00017846994535519127,
      "loss": 0.0009,
      "step": 395
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 0.10407977551221848,
      "learning_rate": 0.00017819672131147543,
      "loss": 0.0014,
      "step": 400
    },
    {
      "epoch": 0.3319672131147541,
      "grad_norm": 0.003155569778755307,
      "learning_rate": 0.00017792349726775957,
      "loss": 0.0005,
      "step": 405
    },
    {
      "epoch": 0.3360655737704918,
      "grad_norm": 0.006420270539820194,
      "learning_rate": 0.00017765027322404374,
      "loss": 0.0005,
      "step": 410
    },
    {
      "epoch": 0.3401639344262295,
      "grad_norm": 0.0073453523218631744,
      "learning_rate": 0.00017737704918032788,
      "loss": 0.0005,
      "step": 415
    },
    {
      "epoch": 0.3442622950819672,
      "grad_norm": 0.015700800344347954,
      "learning_rate": 0.00017710382513661205,
      "loss": 0.0011,
      "step": 420
    },
    {
      "epoch": 0.3483606557377049,
      "grad_norm": 0.04038052260875702,
      "learning_rate": 0.0001768306010928962,
      "loss": 0.0012,
      "step": 425
    },
    {
      "epoch": 0.3524590163934426,
      "grad_norm": 0.0008535035303793848,
      "learning_rate": 0.00017655737704918033,
      "loss": 0.0008,
      "step": 430
    },
    {
      "epoch": 0.35655737704918034,
      "grad_norm": 0.00859803706407547,
      "learning_rate": 0.0001762841530054645,
      "loss": 0.0008,
      "step": 435
    },
    {
      "epoch": 0.36065573770491804,
      "grad_norm": 0.44438108801841736,
      "learning_rate": 0.00017601092896174863,
      "loss": 0.0068,
      "step": 440
    },
    {
      "epoch": 0.36475409836065575,
      "grad_norm": 0.014533805660903454,
      "learning_rate": 0.00017573770491803277,
      "loss": 0.0007,
      "step": 445
    },
    {
      "epoch": 0.36885245901639346,
      "grad_norm": 0.005178009159862995,
      "learning_rate": 0.00017546448087431694,
      "loss": 0.0006,
      "step": 450
    },
    {
      "epoch": 0.3729508196721312,
      "grad_norm": 0.04526493698358536,
      "learning_rate": 0.00017519125683060108,
      "loss": 0.0006,
      "step": 455
    },
    {
      "epoch": 0.3770491803278688,
      "grad_norm": 0.011716095730662346,
      "learning_rate": 0.00017491803278688525,
      "loss": 0.002,
      "step": 460
    },
    {
      "epoch": 0.38114754098360654,
      "grad_norm": 0.0009423488518223166,
      "learning_rate": 0.0001746448087431694,
      "loss": 0.0032,
      "step": 465
    },
    {
      "epoch": 0.38524590163934425,
      "grad_norm": 0.0077596502378582954,
      "learning_rate": 0.00017437158469945356,
      "loss": 0.0011,
      "step": 470
    },
    {
      "epoch": 0.38934426229508196,
      "grad_norm": 0.0035024548415094614,
      "learning_rate": 0.0001740983606557377,
      "loss": 0.0004,
      "step": 475
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 0.18749438226222992,
      "learning_rate": 0.00017382513661202186,
      "loss": 0.0021,
      "step": 480
    },
    {
      "epoch": 0.3975409836065574,
      "grad_norm": 0.0100320503115654,
      "learning_rate": 0.00017355191256830603,
      "loss": 0.0008,
      "step": 485
    },
    {
      "epoch": 0.4016393442622951,
      "grad_norm": 0.0029418293852359056,
      "learning_rate": 0.00017327868852459017,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 0.4057377049180328,
      "grad_norm": 0.0038733775727450848,
      "learning_rate": 0.00017300546448087434,
      "loss": 0.0011,
      "step": 495
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 0.014742834493517876,
      "learning_rate": 0.00017273224043715848,
      "loss": 0.0006,
      "step": 500
    }
  ],
  "logging_steps": 5,
  "max_steps": 3660,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 136741825824768.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
